{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1><center>Statistics</center></h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Types\n",
    "\n",
    "- Quantitative (Numerical): This data type expresses a numerical value. It represents quantities and can be counted, measured, added, subtracted, or divided. There are two subtypes:\n",
    "  - Discrete: Data that can only take certain values, usually counts. Example: Number of dogs.\n",
    "  - Continuous: Data that can take any value within a given range, typically measurements. Example: Age, Income, Height, Travel distance to work, Temperature, Average speed.\n",
    "\n",
    "- Categorical (Qualitative): This data type expresses a characteristic or quality. It represents categories or groups and typically can't be used in mathematical operations. There are two subtypes:\n",
    "  - Nominal: Categories with no order or priority. Example: Breed of dogs, Zip code.\n",
    "  - Ordinal: Categories with a certain order or priority. Example: Martial status (Single, Married, Divorced), Letter grades (A, B, C, D, F), Ratings on a survey (1 to 5)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Describing Quantitative Data\n",
    "\n",
    "Quantitative data can be described using four main aspects:\n",
    "\n",
    "1. Measures of Center\n",
    "2. Measures of Spread\n",
    "3. The Shape\n",
    "4. Outliers\n",
    "\n",
    "## Measures of Center\n",
    "These are statistical measures that identify the center of a dataset.\n",
    "<br>\n",
    "### Mean\n",
    "- The sum of all data points divided by the number of data points.\n",
    "- Highly influenced by outliers.\n",
    "\n",
    "**Note:** The mean isn't always the best measure of center because it is strongly influenced by extreme values and it always returns a decimal value, even for discrete data types.\n",
    "<br>\n",
    "### Median\n",
    "- Middle value in the sorted dataset.\n",
    "- Less influenced by outliers.\n",
    "\n",
    "**Median for Odd Values:** The median is the number in the direct middle when the values are ordered from smallest to largest.\n",
    "\n",
    "**Median for Even Values:** The median is the average of the two values in the middle when the values are ordered from smallest to largest.\n",
    "\n",
    "*To compute the median, the values must be sorted first.*\n",
    "\n",
    "The choice between using the mean or median largely depends on the shape of the dataset and if there are any outliers.\n",
    "<br>\n",
    "### Mode\n",
    "- The most frequently occurring value.\n",
    "- Can be used with categorical data.\n",
    "\n",
    "**Note:** A dataset can have no mode (all observations occur with the same frequency), one mode (one number appears most frequently), or many modes (two or more numbers appear most frequently).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mathematical Notation\n",
    "\n",
    "Mathematical notation is a universal language used by professionals to convey mathematical ideas, similar to how emojis or memes convey feelings. You're likely already familiar with basic notation: `+`, `-`, `×`, `÷`, and `=`.\n",
    "\n",
    "## Why Learn Notation?\n",
    "\n",
    "1. **Makes You Appear Knowledgeable:** Knowing how to read and write in notation is like learning a new language, a language that conveys mathematical ideas.\n",
    "\n",
    "2. **Helps Understand Documentation:** Notation is often used to explain how problems are solved. For example, to truly understand how the Gradient Boosting algorithm works, one must be comfortable with notation.\n",
    "\n",
    "3. **Simplifies Complex Ideas:** When words fall short, notation can clearly convey complex mathematical ideas.\n",
    "\n",
    "# Random Variables\n",
    "\n",
    "A random variable is a placeholder for possible outcomes of a certain process. They are often represented by capital letters such as X, Y, or Z.\n",
    "\n",
    "- **Example 1: X might represent the amount of time someone spends on a website. X is a \"holder\" for values that range from 0 to infinity.\n",
    "\n",
    "- **Example 2: Y might represent whether an individual purchases a product or not.\n",
    "\n",
    "# Random Variables and Observed Values\n",
    "\n",
    "Random variables are represented by capital letters (e.g., X). Observed outcomes of these variables are notated as a lowercase of the same letter with a subscript associated with the observation order (e.g., x1, x2, ...).\n",
    "\n",
    "## Example: Time Spent on Website\n",
    "\n",
    "**Random Variable:** X represents the amount of time an individual spends on the website.\n",
    "\n",
    "**Observed Values:** Let's say we observed five individuals:\n",
    "\n",
    "- The first visitor (x1) spends 10 minutes.\n",
    "- The second visitor (x2) spends 20 minutes.\n",
    "- The third visitor (x3) spends 45 minutes.\n",
    "- The fourth visitor (x4) spends 12 minutes.\n",
    "- The fifth visitor (x5) spends 8 minutes.\n",
    "\n",
    "## Probability\n",
    "\n",
    "We can also express probability using random variables:\n",
    "\n",
    "- **P(X>20):** The probability that the amount of time spent on the website is greater than 20. Here, only one out of five observations exceeds 20, so P(X>20) = 1/5 or 20%.\n",
    "\n",
    "- **P(X≥20):** The probability of an individual spending 20 or more minutes on the website. Two out of five observations meet this condition, so P(X≥20) = 2/5 or 40%.\n",
    "\n",
    "# Notation for Calculating the Mean\n",
    "\n",
    "The mean is calculated as the sum of all values divided by the number of values.\n",
    "\n",
    "In our notation, adding multiple values of a random variable can become tedious.\n",
    "\n",
    "**Example:**\n",
    "- For three values: x1 + x2 + x3\n",
    "- For six values: x1 + x2 + x3 + x4 + x5 + x6\n",
    "\n",
    "**Problem:** Extending this to hundreds, thousands, or millions of values is impractical. We need a simpler way to represent this calculation.\n",
    "\n",
    "# Aggregations\n",
    "\n",
    "Aggregations convert multiple numbers into fewer numbers, commonly a single value.\n",
    "\n",
    "- **Summation (Σ):** Greek symbol sigma. Commonly used aggregation.\n",
    "    - **Example 1:** For sum of first three data points of time spent on website:\n",
    "        - Original: x1 + x2 + x3 = 10 + 20 + 45 = 75\n",
    "        - New Notation: ∑i=1 to 3 (xi) = x1 + x2 + x3\n",
    "    - **Example 2:** For sum of last three data points:\n",
    "        - Original: x7 + x8 + x9\n",
    "        - New Notation: ∑i=7 to 9 (xi) = x7 + x8 + x9\n",
    "\n",
    "- Other Aggregations:\n",
    "    - **Product (Π):** Greek letter pi used for multiplication of all values.\n",
    "    - **Integration (∫):** Used to aggregate continuous values. Common in calculus.\n",
    "\n",
    "# Calculating the Mean\n",
    "\n",
    "The mean of a dataset is calculated using the following formula:\n",
    "\n",
    "- Mean = (1/n) * Σ(xi) from i=1 to n\n",
    "\n",
    "Where:\n",
    "- **n** is the total number of values in the dataset.\n",
    "- **Σ** is the sum of the values.\n",
    "- **xi** represents each data value in the dataset.\n",
    "\n",
    "This is commonly written as **x̄**. Other variables such as **ȳ** can also be used to represent the mean.\n",
    "\n",
    "The letter **i** in the formula is an index and can be replaced with any other letter such as **j**, **k**, or **m**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Measures of Spread\n",
    "\n",
    "# Histograms\n",
    "\n",
    "Histograms are a common way to visually represent quantitative data. They provide insights into:\n",
    "\n",
    "- Center: The middle value(s) in a dataset.\n",
    "- Spread: The range of the data.\n",
    "- Shape: The distribution of the data.\n",
    "- Outliers: The values that are significantly different from the rest of the data.\n",
    "\n",
    "## Constructing a Histogram\n",
    "\n",
    "1. **Binning:** This involves dividing the entire range of values into a series of intervals, or 'bins'.\n",
    "2. **Counting:** The number of data points that fall within each bin is then counted.\n",
    "3. **Plotting:** Each bin is represented by a bar, the height of which corresponds to the count of data points within that bin.\n",
    "\n",
    "Different choices of bin sizes can lead to different visual interpretations of the data. In most cases, software will automatically choose appropriate bin sizes.\n",
    "\n",
    "# Five-Number Summary\n",
    "\n",
    "The five-number summary includes the following statistics:\n",
    "\n",
    "1. **Minimum**: The smallest value in the dataset.\n",
    "2. **Q1**: The 25th percentile; 25% of the data are less than this value.\n",
    "3. **Q2 (Median)**: The 50th percentile; 50% of the data are less than this value.\n",
    "4. **Q3**: The 75th percentile; 75% of the data are less than this value.\n",
    "5. **Maximum**: The largest value in the dataset.\n",
    "\n",
    "These statistics are essentially medians of different subsets of the dataset, and are computed differently based on whether the dataset has an odd or even number of values.\n",
    "\n",
    "Other relevant statistics include:\n",
    "\n",
    "- **Range**: The difference between the maximum and the minimum values.\n",
    "- **Interquartile Range (IQR)**: The difference between Q3 and Q1, representing the range of the middle 50% of the data.\n",
    "\n",
    "# Box Plots\n",
    "\n",
    "Box plots are a type of visual representation that summarize key aspects of a dataset, including the five-number summary (minimum, Q1, median (Q2), Q3, maximum), and are particularly useful for comparing distributions. They are constructed as follows:\n",
    "\n",
    "1. **Whiskers**: These lines extend from the box and represent the minimum and maximum values of the dataset. The length of the whiskers shows the range of the data.\n",
    "\n",
    "2. **Box**: The box itself signifies the interquartile range (IQR). The edges of the box correspond to the first (Q1) and third quartiles (Q3).\n",
    "\n",
    "3. **Line within the box**: This line represents the median (Q2) of the dataset.\n",
    "\n",
    "Box plots can be useful for comparing variability between two data sets, as demonstrated with the comparison between numbers of dogs seen on weekdays and weekends.\n",
    "\n",
    "# Introduction to Variance and Standard Deviation\n",
    "\n",
    "## Other Measures of Spread\n",
    "\n",
    "Apart from the five-number summary (min, Q1, median (Q2), Q3, max) and associated measures of spread (range and IQR), variance and standard deviation are also commonly used to understand the spread of data. These measures are especially useful for datasets that are not symmetric.\n",
    "\n",
    "## Variance\n",
    "\n",
    "The variance measures the average squared difference of each observation from the mean. It is calculated as:\n",
    "\n",
    "    1/n * Σ((x_i - x̄)^2)  for i = 1 to n\n",
    "\n",
    "In a spreadsheet application, this can be calculated in a series of steps:\n",
    "\n",
    "1. Subtract the mean from each data point.\n",
    "2. Square each result.\n",
    "3. Sum all the squared results.\n",
    "4. Divide this sum by the number of data points.\n",
    "\n",
    "## Standard Deviation\n",
    "\n",
    "The standard deviation is the square root of the variance, hence it's in the same unit as the original data. This makes the standard deviation easier to interpret than the variance.\n",
    "\n",
    "The formula for the standard deviation is the square root of the variance:\n",
    "\n",
    "    sqrt(1/n * Σ((x_i - x̄)^2))  for i = 1 to n\n",
    "\n",
    "In a spreadsheet application, the standard deviation can be found by taking the square root of the calculated variance.\n",
    "\n",
    "# Calculating Variance and Standard Deviation with an Example\n",
    "\n",
    "Consider the dataset: 10, 14, 10, 6\n",
    "\n",
    "1. **Calculating the Mean**: Sum all data points and divide by the number of data points:\n",
    "\n",
    "   x̄ = (10+14+10+6)/4 = 10\n",
    "\n",
    "2. **Calculating the difference from the mean**: Subtract the mean from each data point:\n",
    "\n",
    "   10 - 10 = 0\n",
    "   14 - 10 = 4\n",
    "   10 - 10 = 0\n",
    "   6 - 10 = -4\n",
    "\n",
    "3. **Finding the average of these differences**: Add all differences and divide by the number of data points:\n",
    "\n",
    "   (0 + 4 + 0 - 4) / 4 = 0\n",
    "\n",
    "   This results in 0, suggesting there is no spread. To correct for negative values, we square the differences.\n",
    "\n",
    "4. **Calculating Variance**: Square each difference from the mean and find the average of these squared differences:\n",
    "\n",
    "   Variance = 1/n Σ((x_i - x̄)^2) for i = 1 to n\n",
    "   = 1/4 * (0 + 16 + 0 + 16) = 8\n",
    "\n",
    "5. **Calculating Standard Deviation**: Take the square root of the variance:\n",
    "\n",
    "   Standard deviation = sqrt(Variance) = sqrt(8) = 2.83\n",
    "\n",
    "The standard deviation of ~2.83 suggests that on average, each point in our dataset is approximately 2.83 units away from the mean.\n",
    "\n",
    "## Why the Standard Deviation?\n",
    "\n",
    "Standard deviation is a widely used metric to evaluate the spread of datasets. The advantages of using standard deviation over the 5-number summary include:\n",
    "\n",
    "- **Simplicity**: Standard deviation condenses the measure of data spread into a single metric, making it simpler to understand and communicate.\n",
    "- **Utility in Inferential Statistics**: Standard deviation plays a crucial role in various statistical tests and measures, making it indispensable in inferential statistics.\n",
    "\n",
    "## Important Final Points\n",
    "\n",
    "- **Variance**: Variance is used to compare the spread of two different groups. A dataset with a higher variance is more spread out than a dataset with lower variance. However, variance might be inflated by outliers, so interpret with care.\n",
    "- **Units**: When comparing the spread between two datasets, the units of each must be the same.\n",
    "- **Risk**: In fields related to money or economics, higher variance (or standard deviation) is associated with higher risk.\n",
    "- **Standard Deviation**: The standard deviation is used more often in practice than the variance, because it shares the units of the original dataset.\n",
    "\n",
    "## Use in the World\n",
    "\n",
    "The standard deviation plays a vital role in various fields:\n",
    "\n",
    "- **Finance**: It is associated with risk.\n",
    "- **Medical Studies**: It assists in determining the significance of drugs.\n",
    "- **Predictive Models**: It measures the error of our results for predicting anything from the amount of rainfall we can expect tomorrow to your predicted commute time tomorrow.\n",
    "\n",
    "Understanding the spread of a particular set of data is crucial in many areas. In this lesson, you mastered the calculation of the most common measures of spread.\n",
    "\n",
    "## Measures of Center and Spread Summary\n",
    "\n",
    "### Recap\n",
    "\n",
    "#### Calculating Variance\n",
    "The variance can be calculated as:\n",
    "\n",
    "    σ² = (1/n) ∑ (xi−x̄)²   (for a population)\n",
    "    s² = (1/(n-1)) ∑ (xi−x̄)²   (for a sample)\n",
    "\n",
    "\n",
    "#### Standard Deviation vs. Variance\n",
    "The standard deviation is the square root of the variance. In practice, the standard deviation is usually used instead of the variance. This is because the standard deviation shares the same units with our original data, while the variance has squared units.\n",
    "\n",
    "## Bessel's Correction\n",
    "\n",
    "When calculating the standard deviation from a sample, we use a divisor of `n-1` instead of `n`. This correction is known as **Bessel's correction**. The rationale behind this correction is as follows:\n",
    "\n",
    "- The sample mean is typically closer to the observed values than the population mean. Therefore, using the sample mean tends to underestimate the population variance or standard deviation.\n",
    "- To correct this bias, we use `n-1` instead of `n`, which makes the estimated standard deviation a bit larger, thus providing a better estimation of the population standard deviation.\n",
    "- The correction has a larger proportional effect when `n` is small (which is beneficial because the sample mean is likely to be less accurate with a smaller sample).\n",
    "\n",
    "When the sample encompasses the entire population, we use `n` as the divisor since the sample mean is equivalent to the population mean.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Shape And Outliners\n",
    "\n",
    "Data distribution shapes, revealed by histograms, are key to interpreting data measures. Common shapes are:\n",
    "\n",
    "1. **Right-skewed**: Mean > Median (e.g. lifespan of light bulbs)\n",
    "2. **Left-skewed**: Mean < Median (e.g. university grades percentages)\n",
    "3. **Symmetric**: Also known as bell curve or normal distribution, where Mean = Median (e.g. human height)\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Shape | Mean vs. Median |\n",
    "|-------|-----------------|\n",
    "| Symmetric (Normal) | Mean equals Median |\n",
    "| Right-skewed | Mean greater than Median |\n",
    "| Left-skewed | Mean less than Median |\n",
    "\n",
    "**Note**: The 'mode' is the tallest histogram bar. A distribution can have multiple modes.\n",
    "\n",
    "## Visualizing Data Shapes\n",
    "\n",
    "Quickly plotting your data can reveal its distribution shape. Some common shapes and examples are:\n",
    "\n",
    "1. **Bell Shaped (Symmetric)**: This shape, also known as a normal distribution, is common in data such as human heights, weights, and standardized test scores.\n",
    "\n",
    "2. **Left Skewed**: This shape often appears in data where lower values are more frequent, such as GPA (most students have higher grades), age at death in developed countries (most people live into old age), and changes in asset prices (which tend to go up more often than they go down).\n",
    "\n",
    "3. **Right Skewed**: This shape is typical in data where higher values are more frequent, such as wealth distribution (few people own most of the wealth), or athletic abilities (few athletes perform at the top level).\n",
    "\n",
    "## Handling Outliers\n",
    "\n",
    "Outliers are data points that significantly differ from the rest of the data. They can greatly influence measures such as the mean and standard deviation, more so than measures related to the five-number summary.\n",
    "\n",
    "### Identifying Outliers\n",
    "\n",
    "Outliers can be identified using various techniques. A detailed paper on this topic is available [http://d-scholarship.pitt.edu/7948/1/Seo.pdf]. Generally, a simple visual inspection of the data (e.g., through a scatterplot or boxplot) can help in detecting suspicious points which may be outliers. Always remember, outliers might not always be 'bad' data, they could also represent important aspects of the variation in your data.\n",
    "\n",
    "![Box_Plot](./images/1.png)\n",
    "![Box_Plot_Answers](./images/2.png)\n",
    "![Box_Plot_2Answers](./images/3.png)\n",
    "\n",
    "## Working with Outliers\n",
    "\n",
    "When outliers are present in the data, consider the following steps:\n",
    "\n",
    "1. **Acknowledge their existence**: Understand the impact of outliers on summary statistics.\n",
    "\n",
    "2. **Error correction**: If the outliers are due to typographical errors, remove or correct these values.\n",
    "\n",
    "3. **Understand their cause**: Comprehend why outliers exist and how they impact the questions you're trying to answer about your data.\n",
    "\n",
    "4. **Use appropriate statistics**: The five-number summary values often provide a better indication of data distribution than mean and standard deviation when outliers are present.\n",
    "\n",
    "5. **Careful reporting**: Ensure you're asking the right questions when interpreting and reporting your data analysis.\n",
    "\n",
    "## Anomaly Detection\n",
    "\n",
    "Anomaly detection is a technique used to identify unusual patterns or outliers in data that deviate from expected behavior. It's widely used for fraud detection, system health monitoring, fault detection, and detecting ecosystem disturbances.\n",
    "\n",
    "1. **Statistical Anomaly Detection**: Involves building a statistical model for normal behavior and then applying a statistical test to identify outliers.\n",
    "\n",
    "2. **Machine Learning-Based Anomaly Detection**: Involves training a model to learn the normal pattern in the data and then using it to detect anomalies. Common methods include clustering, classification, and nearest neighbors techniques.\n",
    "\n",
    "3. **Time Series Anomaly Detection**: Common in monitoring applications where you need to detect anomalies in time-series data. Techniques can include statistical process control and decomposing the series into trend, seasonality, and residuals.\n",
    "\n",
    "4. **Deep Learning-Based Anomaly Detection**: Deep learning methods can also be used for anomaly detection. Autoencoders are a common method for this.\n",
    "\n",
    "Remember, the goal of anomaly detection is not just to detect outliers but to understand their causes and take appropriate action.\n",
    "\n",
    "## Handling Outliers - Advice\n",
    "\n",
    "When working with any column (random variable) in your dataset, consider the following steps:\n",
    "\n",
    "1. **Plot your data**: This can help identify outliers visually.\n",
    "\n",
    "2. **Handle outliers**: Depending on the context, outliers can be removed, corrected or left as is. The appropriate treatment often depends on the reasons why the outliers exist.\n",
    "\n",
    "3. **Distribution of your data**: If no outliers are present and your data follows a normal distribution, use the mean and standard deviation to describe your dataset. In such a case, report that the data are normally distributed.\n",
    "\n",
    "   - **Side note**: If you aren't sure if your data are normally distributed, there are tools to help, such as normal quantile plots or statistical tests like the Kolmogorov-Smirnov test.\n",
    "\n",
    "4. **Skewed data or outliers**: If your data are skewed or have outliers, use the five-number summary to summarize your data and report the presence of outliers.\n",
    "\n",
    "## Kolmogorov-Smirnov Test\n",
    "\n",
    "The Kolmogorov-Smirnov (K-S) test is a non-parametric test used to compare a sample with a reference probability distribution, or to compare two samples. It is particularly useful for checking if a sample follows a specific distribution.\n",
    "\n",
    "The K-S test calculates the maximum difference (D) between the cumulative distribution function (CDF) of the observed data and the CDF of the reference distribution or between the CDFs of two empirical distributions. The null hypothesis is that the sample is drawn from the reference distribution. If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same.\n",
    "\n",
    "## Quantile-Quantile (QQ) Plots\n",
    "\n",
    "A QQ Plot is a graphical tool used to check if a dataset follows a particular theoretical distribution. It is created by plotting the quantiles of the data against the quantiles of the chosen distribution.\n",
    "\n",
    "If the data follow the chosen distribution, the points in the QQ plot will approximately lie on the line y = x. If the data are not distributed in the same way as the theoretical distribution, the points in the QQ plot will deviate from the y = x line.\n",
    "\n",
    "QQ plots are used extensively in exploratory data analysis and in the assumptions testing phase of the statistical modelling to verify the normality of the residuals."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Histogram](./images/4.png)\n",
    "![Histogram2](./images/5.png)\n",
    "\n",
    "## Review: Shape, Outliers, Histograms and Box Plots\n",
    "\n",
    "### Shape of Data\n",
    "\n",
    "Data distributions often align with one of the three shapes:\n",
    "\n",
    "1. Right-skewed\n",
    "2. Left-skewed\n",
    "3. Symmetric (often normally distributed)\n",
    "\n",
    "The choice between different measures of center or spread depends on the shape of the dataset.\n",
    "\n",
    "- For normally distributed data, the mean and standard deviation suffice.\n",
    "- For skewed datasets, the 5 number summary may be more suitable.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Outliers significantly influence measures like the mean. Approach outliers depending on the context:\n",
    "\n",
    "1. Acknowledge their existence and impact on summary statistics.\n",
    "2. If they result from typos - fix or remove.\n",
    "3. Understand why they exist and their impact on the study.\n",
    "4. In the presence of outliers, the 5 number summary may be more informative than the mean and standard deviation.\n",
    "5. Be cautious while reporting. Ask the right questions.\n",
    "\n",
    "### Histograms and Box Plots\n",
    "\n",
    "Histograms and box plots are essential for visualizing quantitative data. Identifying outliers and understanding data distribution become more straightforward visually than through summary statistics.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction to Probability and Statistics\n",
    "\n",
    "- **Probability**: It's a predictive approach, where we make forecasts about future events based on assumptions or models.\n",
    "\n",
    "- **Statistics**: It's an analytical approach, where we examine data from past events to infer possible models or causes.\n",
    "\n",
    "## Probability Formula\n",
    "\n",
    "The formula to calculate probability of an event 'A' can also be expressed as:\n",
    "\n",
    "    P(A) = 1 - P(~A)\n",
    "\n",
    "This reads as \"The probability of 'A' is equal to 1 minus the probability of 'not A'.\"\n",
    "\n",
    "## Truth Table for Coin Flips\n",
    "\n",
    "A truth table can help us understand probabilities better. Consider a coin flip scenario:\n",
    "\n",
    "| Flip 1 | Flip 2 | Probability of Each Outcome |\n",
    "|--------|--------|-----------------------------|\n",
    "| H      | H      | 0.25                        |\n",
    "| H      | T      | 0.25                        |\n",
    "| T      | H      | 0.25                        |\n",
    "| T      | T      | 0.25                        |\n",
    "\n",
    "Another way to calculate the probability of getting heads two times in a row (H, H) is by multiplying the individual probabilities:\n",
    "\n",
    "    P(H,H) = P(H) * P(H)\n",
    "\n",
    "For a fair coin, P(H) is 0.5, so:\n",
    "\n",
    "    P(H,H) = 0.5 * 0.5 = 0.25\n",
    "\n",
    "## Probability Fundamentals\n",
    "\n",
    "### Definitions\n",
    "- **Probability**: Making predictions about future events based on models or assumptions.\n",
    "- **Statistics**: Analyzing the data from past events to infer what the models or causes could have been.\n",
    "\n",
    "### Basic Rules of Probability\n",
    "\n",
    "1. **Probability of an Event**: Denoted by P(E), where E is the event. For example, for a fair coin, P(Heads) = 0.5.\n",
    "\n",
    "2. **Complementary Probability**: The probability of the complement of an event (¬E) is 1 minus the probability of the event itself, i.e., P(¬E) = 1 - P(E). For instance, the probability of not getting heads in a coin flip is 1 - P(Heads) = 0.5.\n",
    "\n",
    "3. **Sum of Probabilities**: The sum of probabilities for all possible outcomes of an event equals 1.\n",
    "\n",
    "4. **Independent Events**: If events are independent, the probability of all events occurring is the product of their individual probabilities. This is represented as P(A ∩ B) = P(A) * P(B).\n",
    "\n",
    "These rules, however, do not apply when events are dependent, such as predicting the probability of it raining tomorrow given that it rained today.\n",
    "\n",
    "After this lesson, you should be able to:\n",
    "\n",
    "- Describe and calculate the probability of one event given another.\n",
    "- Calculate the complement of an event.\n",
    "- Calculate the probability of composite events.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Binomial Distributions: Introduction\n",
    "\n",
    "In this lesson, **Sebastian** introduces us to **Binomial Distribution**.\n",
    "\n",
    "### Key Definition:\n",
    "**Binomial Distribution**: A statistical distribution predicting the likelihood of one of two outcomes (similar to a coin flip) across a sequence of independent events.\n",
    "\n",
    "> **Note**: The independence of events under discussion remains a constant. All previously learned rules still hold.\n",
    "\n",
    "## Example: Probability of 4 Heads in 10 Coin Flips\n",
    "\n",
    "Let's calculate the probability of obtaining exactly 4 Heads in 10 coin flips.\n",
    "\n",
    "- The number of unique positions available for each Head is as follows:\n",
    "\n",
    "| Head | Positions Available |\n",
    "|------|---------------------|\n",
    "| 1st  | 10                  |\n",
    "| 2nd  | 9                   |\n",
    "| 3rd  | 8                   |\n",
    "| 4th  | 7                   |\n",
    "\n",
    "Multiplying these positions (10 * 9 * 8 * 7 = 5040) leads to duplication.\n",
    "\n",
    "- To avoid duplications, we need to account for the number of unique positions already occupied:\n",
    "\n",
    "| Head | Duplications |\n",
    "|------|--------------|\n",
    "| 1st  | 4            |\n",
    "| 2nd  | 3            |\n",
    "| 3rd  | 2            |\n",
    "| 4th  | 1            |\n",
    "\n",
    "Now, divide the total number of positions by the product of the duplications to remove these: 5040 / (4 * 3 * 2 * 1) = 210.\n",
    "\n",
    "Therefore, out of 1024 total outcomes (2 to the power of 10), 210 will result in exactly 4 Heads and 6 Tails.\n",
    "\n",
    "## Binomial Counting Using Factorials\n",
    "\n",
    "Factorials are a handy tool in combinatorics and can be represented as `n! = n*(n-1)*(n-2)*...*1`.\n",
    "\n",
    "For instance:\n",
    "5! = 5 * 4 * 3 * 2 * 1\n",
    "\n",
    "Factorials can be used to compute the combinations in a binomial distribution where `n` is the number of trials (coin flips) and `k` is the number of successes (Heads).\n",
    "\n",
    "If `n = 10` (coin flips) and `k = 5` (Heads), you might initially consider the following:\n",
    "10 * 9 * 8 * 7 * 6 / 5 * 4 * 3 * 2 * 1\n",
    "\n",
    "However, the accurate factorial representation is `n! / (k! * (n - k)!)`.\n",
    "\n",
    "This equation correctly computes the number of ways `k` successes can occur in `n` trials, without over-counting.\n",
    "\n",
    "## Probability Recap\n",
    "\n",
    "Remember:\n",
    "\n",
    "1. Probabilities are numbers between 0 and 1.\n",
    "2. The size of the Truth Table for coin flips is calculated by raising the number of outcomes (Heads/Tails = 2) to the power of the number of flips. For instance, 10 flips will yield a Truth Table of size `2^10` or 1024.\n",
    "\n",
    "Consider 5 fair coin flips:\n",
    "\n",
    "- `P(H) = 0.5`\n",
    "- `P(T) = 0.5`\n",
    "\n",
    "To calculate the probability of a specific outcome:\n",
    "\n",
    "1. Compute `n! / (k! * (n - k)!)`\n",
    "2. Divide the result by the Truth Table size.\n",
    "\n",
    "## Quiz\n",
    "\n",
    "**Question:** What is the probability of obtaining exactly 1 Head in 5 coin flips?\n",
    "\n",
    "*Hint*: Use the steps above to calculate.\n",
    "\n",
    "**Answer:** 0.15625\n",
    "\n",
    "To elaborate, with 5 coin flips and 1 Head, the factorial computation gives `5! / (1! * (5 - 1)!) = 5`. Given a Truth Table size of `2^5 = 32`, the probability is `5 / 32` or 0.15625. This means that there's a 15.625% chance of getting exactly 1 Head in 5 coin flips.\n",
    "\n",
    "## Probability Calculation for 3 Heads in 5 Coin Flips\n",
    "\n",
    "If we are interested in finding the probability of obtaining exactly 3 Heads in 5 coin flips, we would still use a Truth Table size of `2^5`, as each flip has 2 possible outcomes (Head or Tail).\n",
    "\n",
    "To calculate this probability:\n",
    "\n",
    "1. Compute the number of ways to get 3 Heads in 5 flips, which is `5! / (3! * (5 - 3)!)`.\n",
    "2. Divide this by the Truth Table size (`2^5`).\n",
    "\n",
    "This calculation gives us the probability of getting exactly 3 Heads in 5 coin flips.\n",
    "\n",
    "# Overview\n",
    "\n",
    "\n",
    "The **Binomial Distribution** provides a method to calculate the probability of a sequence of independent 'coin flip' like events. This lesson focused on using the binomial distribution formula to find the probability of any event with two possible outcomes.\n",
    "\n",
    "From this lesson, you should now be able to:\n",
    "\n",
    "- Describe and calculate Binomial Distributions.\n",
    "- Calculate the probabilities of multiple independent events.\n",
    "\n",
    "You have learned how to find the probability of getting `k` Heads in `n` coin flips.\n",
    "\n",
    "The key formulas used are:\n",
    "\n",
    "- Number of ways to get `k` Heads in `n` coin flips: `n! / (k! * (n - k)!)`\n",
    "- Probability of `k` Heads and `n-k` Tails: `p^k * (1 - p)^(n - k)`\n",
    "- Binomial Distribution Formula: `(n! / (k! * (n - k)!)) * p^k * (1 - p)^(n - k)`\n",
    "\n",
    "Where:\n",
    "- `n` is the total number of trials (flips),\n",
    "- `k` is the number of successful outcomes we are interested in (Heads),\n",
    "- `p` is the probability of a single successful outcome (probability of Heads).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cancer Probability Example\n",
    "\n",
    "Consider a scenario where we are analyzing the probability related to a cancer diagnosis:\n",
    "\n",
    "- `P(C)`: Probability of having cancer (represented as `P0`). Given as 0.1 or 10%.\n",
    "- `P(Pos|C)`: Probability of testing positive for cancer, given that an individual has cancer (represented as `P1`). This is often referred to as the sensitivity of the test. Given as 0.9 or 90%.\n",
    "- `P(Neg|¬C)`: Probability of testing negative for cancer, given that an individual does not have cancer (represented as `P2`). This is often referred to as the specificity of the test. Given as 0.8 or 80%.\n",
    "\n",
    "| Case | Probability | Sensitivity | Specificity |\n",
    "|------|-------------|-------------|-------------|\n",
    "| P0   | 0.1         | -           | -           |\n",
    "| P1   | -           | 0.9         | -           |\n",
    "| P2   | -           | -           | 0.8         |\n",
    "\n",
    "From the table above, we see that 10% of the general population has cancer, 90% of those with cancer will test positive, and 80% of those without cancer will test negative.\n",
    "\n",
    "Our question: What is the probability that a random individual from the general population will test positive for cancer, irrespective of whether they have cancer or not?\n",
    "\n",
    "We denote this as `P(Pos)`, the probability of a positive test.\n",
    "\n",
    "By leveraging the law of total probability, we can express this as:\n",
    "\n",
    "`P(Pos) = P(C) * P(Pos|C) + P(¬C) * P(Pos|¬C)`\n",
    "\n",
    "Knowing that:\n",
    "\n",
    "- `P(¬C)` is the probability of not having cancer, which equals `1 - P(C) = 1 - 0.1`\n",
    "- `P(Pos|¬C)` is the probability of a positive test for someone who does not have cancer, which equals `1 - P(Neg|¬C) = 1 - 0.8`\n",
    "\n",
    "We can substitute these into our equation to find:\n",
    "\n",
    "`P(Pos) = 0.1 * 0.9 + (1 - 0.1) * (1 - 0.8) = 0.27`\n",
    "\n",
    "So, the probability that a randomly selected individual from the general population will test positive for cancer is 0.27, or 27%.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional Probability in Cancer Diagnosis\n",
    "\n",
    "Consider this hypothetical scenario to better understand Conditional Probability.\n",
    "\n",
    "In a given population, an individual can either have cancer (`Cancer`) or not have cancer (`¬Cancer`).\n",
    "\n",
    "- `P(Cancer) = 0.1` (10% of people have cancer)\n",
    "- `P(¬Cancer) = 0.9` (90% of people do not have cancer)\n",
    "\n",
    "Now, suppose we have a medical test to detect cancer. This introduces the concept of **Conditional Probabilities**. A test result could be either positive (`Positive`) or negative (`Negative`), and these results are contingent on whether the person has cancer or not.\n",
    "\n",
    "For example: `P(Positive|Cancer) = 0.9`\n",
    "\n",
    "This represents the probability of a positive test, given that someone has cancer, which is 0.9 or 90%. The pipe/bar, `|`, denotes that the outcome of the first event depends on the second event.\n",
    "\n",
    "Consider the following table from the video:\n",
    "\n",
    "| Case                    | Probability |\n",
    "|-------------------------|-------------|\n",
    "| P(Cancer)               | 0.1         |\n",
    "| P(¬Cancer)              | 0.9         |\n",
    "| P(Positive&#124;Cancer) | 0.9         |\n",
    "    | P(Negative&#124;Cancer) | 0.1         |\n",
    "        | P(Positive&#124;¬Cancer)| 0.2         |\n",
    "            | P(Negative&#124;¬Cancer)| ?           |\n",
    "\n",
    "            **Quiz Question**\n",
    "\n",
    "                   Given the information and table above, what is the probability that someone tests Negative for cancer and does not have cancer?\n",
    "\n",
    "**Answer:** The probability is 0.8. We can calculate this as follows:\n",
    "\n",
    "`P(Negative|¬Cancer) = 1 - P(Positive|¬Cancer)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a Truth Table for Conditional Probability\n",
    "\n",
    "Consider the table from the previous section:\n",
    "\n",
    "| Case                    | Probability |\n",
    "|-------------------------|-------------|\n",
    "| P(Cancer)               | 0.1         |\n",
    "| P(¬Cancer)              | 0.9         |\n",
    "| P(Positive&#124;Cancer) | 0.9         |\n",
    "    | P(Negative&#124;Cancer) | 0.1         |\n",
    "        | P(Positive&#124;¬Cancer)| 0.2         |\n",
    "            | P(Negative&#124;¬Cancer)| 0.8         |\n",
    "\n",
    "With this information, we can start building a Truth Table for these cases:\n",
    "\n",
    "| Case | Has Cancer (Y/N) | Test Result (P/N) | Probability |\n",
    "|------|------------------|-------------------|-------------|\n",
    "| 1    | Y                | P                 | ?           |\n",
    "| 2    | Y                | N                 | ?           |\n",
    "| 3    | N                | P                 | ?           |\n",
    "| 4    | N                | N                 | ?           |\n",
    "\n",
    "Next, we need to determine the probability for each of these cases.\n",
    "\n",
    "Let's start with Case 1: A person has cancer and the test is positive.\n",
    "\n",
    "**Solution:** The probability is 0.09.\n",
    "\n",
    "We calculated this by multiplying:\n",
    "\n",
    "`P(Cancer) * P(Positive|Cancer) = 0.1 * 0.9 = 0.09`\n",
    "\n",
    "Or in words, the probability of having cancer (0.1) times the probability of testing positive given the person has cancer (0.9).\n",
    "\n",
    "## Building a Truth Table for Conditional Probability (continued)\n",
    "\n",
    "Refer to the table from the previous section:\n",
    "\n",
    "| Case                    | Probability |\n",
    "|-------------------------|-------------|\n",
    "| P(Cancer)               | 0.1         |\n",
    "| P(¬Cancer)              | 0.9         |\n",
    "| P(Positive&#124;Cancer) | 0.9         |\n",
    "| P(Negative&#124;Cancer) | 0.1         |\n",
    "| P(Positive&#124;¬Cancer)| 0.2         |\n",
    "| P(Negative&#124;¬Cancer)| 0.8         |\n",
    "\n",
    "We can continue to build the Truth Table:\n",
    "\n",
    "| Case | Has Cancer (Y/N) | Test Result (P/N) | Probability |\n",
    "|------|------------------|-------------------|-------------|\n",
    "| 1    | Y                | P                 | 0.09        |\n",
    "| 2    | Y                | N                 | ?           |\n",
    "| 3    | N                | P                 | ?           |\n",
    "| 4    | N                | N                 | ?           |\n",
    "\n",
    "Next, let's calculate for Case 2: A person has cancer and the test is negative.\n",
    "\n",
    "**Solution:** The probability is 0.01.\n",
    "\n",
    "We calculated this by multiplying:\n",
    "\n",
    "`P(Cancer) * P(Negative|Cancer) = 0.1 * 0.1 = 0.01`\n",
    "\n",
    "In other words, the probability of having cancer (0.1) times the probability of testing negative given the person has cancer (0.1).\n",
    "\n",
    "## Building a Truth Table for Conditional Probability (continued)\n",
    "\n",
    "Refer to the table from the previous section:\n",
    "\n",
    "| Case                    | Probability |\n",
    "|-------------------------|-------------|\n",
    "| P(Cancer)               | 0.1         |\n",
    "| P(¬Cancer)              | 0.9         |\n",
    "| P(Positive&#124;Cancer) | 0.9         |\n",
    "| P(Negative&#124;Cancer) | 0.1         |\n",
    "| P(Positive&#124;¬Cancer)| 0.2         |\n",
    "| P(Negative&#124;¬Cancer)| 0.8         |\n",
    "\n",
    "We can continue to build the Truth Table:\n",
    "\n",
    "| Case | Has Cancer (Y/N) | Test Result (P/N) | Probability |\n",
    "|------|------------------|-------------------|-------------|\n",
    "| 1    | Y                | P                 | 0.09        |\n",
    "| 2    | Y                | N                 | 0.01        |\n",
    "| 3    | N                | P                 | ?           |\n",
    "| 4    | N                | N                 | ?           |\n",
    "\n",
    "Next, let's calculate for Case 3: A person does not have cancer and the test is positive.\n",
    "\n",
    "**Solution:** The probability is 0.18.\n",
    "\n",
    "We calculated this by multiplying:\n",
    "\n",
    "`P(¬Cancer) * P(Positive|¬Cancer) = 0.9 * 0.2 = 0.18`\n",
    "\n",
    "In other words, the probability of not having cancer (0.9) times the probability of testing positive given the person does not have cancer (0.2).\n",
    "\n",
    "## Building a Truth Table for Conditional Probability (continued)\n",
    "\n",
    "Refer to the table from the previous sections:\n",
    "\n",
    "| Case                    | Probability |\n",
    "|-------------------------|-------------|\n",
    "| P(Cancer)               | 0.1         |\n",
    "| P(¬Cancer)              | 0.9         |\n",
    "| P(Positive&#124;Cancer) | 0.9         |\n",
    "| P(Negative&#124;Cancer) | 0.1         |\n",
    "| P(Positive&#124;¬Cancer)| 0.2         |\n",
    "| P(Negative&#124;¬Cancer)| 0.8         |\n",
    "\n",
    "We can continue to build the Truth Table:\n",
    "\n",
    "| Case | Has Cancer (Y/N) | Test Result (P/N) | Probability |\n",
    "|------|------------------|-------------------|-------------|\n",
    "| 1    | Y                | P                 | 0.09        |\n",
    "| 2    | Y                | N                 | 0.01        |\n",
    "| 3    | N                | P                 | 0.18        |\n",
    "| 4    | N                | N                 | ?           |\n",
    "\n",
    "Lastly, let's calculate for Case 4: A person does not have cancer and the test is negative.\n",
    "\n",
    "**Solution:** The probability is 0.72.\n",
    "\n",
    "We calculated this by multiplying:\n",
    "\n",
    "`P(¬Cancer) * P(Negative|¬Cancer) = 0.9 * 0.8 = 0.72`\n",
    "\n",
    "In other words, the probability of not having cancer (0.9) times the probability of testing negative given the person does not have cancer (0.8).\n",
    "\n",
    "## Building a Truth Table for Conditional Probability (conclusion)\n",
    "\n",
    "Refer to the table from the previous sections:\n",
    "\n",
    "| Case                    | Probability |\n",
    "|-------------------------|-------------|\n",
    "| P(Cancer)               | 0.1         |\n",
    "| P(¬Cancer)              | 0.9         |\n",
    "| P(Positive&#124;Cancer) | 0.9         |\n",
    "| P(Negative&#124;Cancer) | 0.1         |\n",
    "| P(Positive&#124;¬Cancer)| 0.2         |\n",
    "| P(Negative&#124;¬Cancer)| 0.8         |\n",
    "\n",
    "We now have all of the information for the Truth Table:\n",
    "\n",
    "| Case | Has Cancer (Y/N) | Test Result (P/N) | Probability |\n",
    "|------|------------------|-------------------|-------------|\n",
    "| 1    | Y                | P                 | 0.09        |\n",
    "| 2    | Y                | N                 | 0.01        |\n",
    "| 3    | N                | P                 | 0.18        |\n",
    "| 4    | N                | N                 | 0.72        |\n",
    "\n",
    "Adding all these probabilities should sum to 1:\n",
    "\n",
    "`0.09 + 0.01 + 0.18 + 0.72 = 1`\n",
    "\n",
    "This is because the four cases listed in the table exhaust all possibilities - every possible outcome falls into one of these four cases.\n",
    "\n",
    "## Computing Probability of a Positive Test\n",
    "\n",
    "Refer back to the filled Truth Table:\n",
    "\n",
    "| Case | Has Cancer (Y/N) | Test Result (P/N) | Probability |\n",
    "|------|------------------|-------------------|-------------|\n",
    "| 1    | Y                | P                 | 0.09        |\n",
    "| 2    | Y                | N                 | 0.01        |\n",
    "| 3    | N                | P                 | 0.18        |\n",
    "| 4    | N                | N                 | 0.72        |\n",
    "\n",
    "If we want to know the probability of getting a positive test, we need to sum the probabilities of all cases where the test result is positive. In this table, those are cases 1 and 3.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "`0.09 (case 1) + 0.18 (case 3) = 0.27`\n",
    "\n",
    "So, there is a 0.27 or 27% chance that a randomly selected person from the population will test positive for cancer.\n",
    "\n",
    "## Total Probability\n",
    "\n",
    "We denote the probabilities as:\n",
    "\n",
    "- P(C): Probability of having cancer\n",
    "- P(¬C): Probability of not having cancer\n",
    "- P(P|C): Probability of testing positive given the person has cancer\n",
    "- P(N|C): Probability of testing negative given the person has cancer\n",
    "- P(P|¬C): Probability of testing positive given the person doesn't have cancer\n",
    "- P(N|¬C): Probability of testing negative given the person doesn't have cancer\n",
    "\n",
    "The total probability of testing positive, P(P), is calculated using the formula:\n",
    "\n",
    "`P(P) = P(P|C) * P(C) + P(P|¬C) * P(¬C)`\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conditional Probability\n",
    "\n",
    "In real-world situations, the outcome of one event often depends on the outcome of a previous event. This is different from independent events like coin flips or dice rolls, where one event does not influence the outcome of the other. This dependency is represented using conditional probabilities.\n",
    "\n",
    "Consider a case where we are interested in the probability of testing positive for a disease, given that a person has the disease.\n",
    "\n",
    "In mathematical notation, we can represent it as:\n",
    "\n",
    "`P(Positive | Disease)`\n",
    "\n",
    "Here, \"|\" symbol stands for \"given\" and it reads as \"Probability of testing positive given the person has the disease\".\n",
    "\n",
    "The general formula for calculating conditional probability is:\n",
    "\n",
    "`P(A | B) = P(A ∩ B) / P(B)`\n",
    "\n",
    "Applying it to our case:\n",
    "\n",
    "`P(Positive | Disease) = P(Positive ∩ Disease) / P(Disease)`\n",
    "\n",
    "This means the probability of a positive test, given that a person has the disease, is equal to the probability of both having the disease and testing positive, divided by the overall probability of having the disease.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Facebook Question\n",
    "\n",
    "# Bayesian Analysis on Friends' Weather Predictions\n",
    "\n",
    "Let's use Bayesian Analysis to evaluate how reliable our friends' predictions are about whether it's raining in Seoul. Our friends can either tell the truth or lie. The information at hand is:\n",
    "\n",
    "- Probability a friend tells the truth (`p(truth)`) = 2/3\n",
    "- Probability a friend lies (`p(lie)`) = 1 - p(truth) = 1/3\n",
    "\n",
    "## Problem\n",
    "\n",
    "We asked 3 friends if it's raining in Seoul. All of them say \"Yes\".\n",
    "\n",
    "## Using Bayes' Theorem\n",
    "\n",
    "To find out the probability it's actually raining, we apply Bayes' theorem. We denote:\n",
    "\n",
    "- `R` as the event that it's actually raining\n",
    "- `T` as the event that all three friends tell us it's raining\n",
    "\n",
    "The goal is to calculate `p(R|T)`, the probability that it's raining given all three friends say so.\n",
    "\n",
    "According to Bayes' theorem:\n",
    "\n",
    "`p(R|T) = [p(T|R) * p(R)] / p(T)`\n",
    "\n",
    "where:\n",
    "\n",
    "- `p(T|R)`: Probability all three friends say it's raining, given that it is. If a friend tells the truth with probability 2/3, this equals `(2/3)^3 = 8/27`.\n",
    "- `p(R)`: Prior probability it's raining, which we don't know and leave as `p(R)`.\n",
    "- `p(T)`: Total probability all three friends say it's raining. This can be split into the case where it's raining and all friends tell the truth, and the case where it's not raining and all friends lie.\n",
    "\n",
    "Hence:\n",
    "\n",
    "`p(T) = p(T|R) * p(R) + p(T|¬R) * p(¬R) = (8/27) * p(R) + (1/3)^3 * (1 - p(R))`\n",
    "\n",
    "Substitute these into Bayes' theorem:\n",
    "\n",
    "`p(R|T) = [(8/27) * p(R)] / [(8/27) * p(R) + (1/3)^3 * (1 - p(R))]`\n",
    "\n",
    "Without a specific value for `p(R)`, we cannot solve for a numeric value, but this formula demonstrates how we can update our belief about the weather based on the friends' answers.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Conditional Probability** refers to the probability of an event given that another event has occurred. This helps us calculate probabilities of multiple dependent events.\n",
    "\n",
    "Concepts covered:\n",
    "\n",
    "1. **Conditional Probabilities (P(A|B))**: This is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "2. **Dependent Events**: These are events where the occurrence of one event does affect the occurrence of the other(s). This contrasts with independent events, where the occurrence of one event does not influence the occurrence of the other(s).\n",
    "\n",
    "Example of a Conditional Probability:\n",
    "\n",
    "- If we want to calculate the probability that it will rain tomorrow given that it rained today, we are dealing with conditional probability. This is because the probability of rain tomorrow might be affected by the fact that it rained today.\n",
    "\n",
    "# Explanation for \"Statistics How To Article with Examples\n",
    "\n",
    "This is a reference that provides a comprehensive understanding of various probability concepts, including conditional probability. Each concept is explained in a simple language and then followed by a practical example to illustrate the application of the theory.\n",
    "\n",
    "A core concept explained is how to calculate the conditional probability, denoted as P(A|B), which is the probability of event A occurring given that event B has already happened. An example provided is finding the probability that a card drawn from a deck is an ace given that it's a red card.\n",
    "\n",
    "For dependent events, where the occurrence of one event affects the probability of another, we use the formula for conditional probability: P(A|B) = P(A ∩ B) / P(B). In the card example, it would be the probability of drawing a red ace (A) divided by the probability of drawing a red card (B).\n",
    "\n",
    "# Explanation for \"Interactive Conditional Probability Visual\"\n",
    "\n",
    "This is an interactive online tool that visually explains how conditional probability works. It uses colored circles to represent different events and their intersections, which aids in understanding of complex, dependent events.\n",
    "\n",
    "The tool allows you to adjust parameters and see in real-time how it affects the probability of the events. It demonstrates that the probability of an event A, given that another event B has occurred, can be very different from the initial probability of A. This tool effectively shows how the occurrence of one event can affect the probability of another, a key concept in conditional probability.\n",
    "\n",
    "The tool provides a more intuitive and visual way to grasp the concept of conditional probability, making it an excellent resource for those who prefer visual learning."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Cancer Test Case Probabilities\n",
    "\n",
    "Let's define the given probabilities:\n",
    "\n",
    "- **P(Cancer) = 0.01**: This represents the probability that a person has cancer.\n",
    "- **P(¬Cancer) = 0.99**: This represents the probability that a person doesn't have cancer.\n",
    "- **P(Positive|Cancer) = 0.9**: This represents the probability that a person with cancer tests positive. This is also known as Sensitivity.\n",
    "- **P(Negative|¬Cancer) = 0.9**: This represents the probability that a person without cancer tests negative. This is also known as Specificity.\n",
    "\n",
    "Now, we want to determine: **P(Cancer|Positive)**, the probability that a person really has cancer given a positive test result.\n",
    "\n",
    "First, we need to consider our intuition. We know that only 1% of people have cancer. If a person has already tested positive, the likelihood of that person having cancer will be higher than 1%, but by how much?\n",
    "\n",
    "To calculate this, we use the formula of conditional probability:\n",
    "\n",
    "> P(Cancer|Positive) = P(Cancer & Positive) / P(Positive)\n",
    "\n",
    "Where:\n",
    "- P(Cancer & Positive) is the probability that a person who tests positive actually has cancer\n",
    "- P(Positive) is the probability that a person tests positive, regardless of whether they have cancer or not.\n",
    "\n",
    "The denominator, P(Positive), can be further broken down into the sum of two probabilities:\n",
    "1. The probability of a person having cancer and testing positive [P(Cancer & Positive)]\n",
    "2. The probability of a person not having cancer but still testing positive [P(¬Cancer & Positive)]\n",
    "\n",
    "Thus, we rewrite the equation as follows:\n",
    "\n",
    "> P(Cancer|Positive) = P(Cancer & Positive) / [P(Cancer & Positive) + P(¬Cancer & Positive)]\n",
    "\n",
    "Most people (99%) don't have cancer, which means that the number of people who test positive and have cancer [P(Cancer & Positive)] is small compared to the number of people who test positive but don't have cancer [P(¬Cancer & Positive)]. Hence, the ratio (and thus, P(Cancer|Positive)) will be less than 90%.\n",
    "\n",
    "Given the options above, the best estimate is that P(Cancer|Positive) is higher than 1% and lower than 90%. This leads us to choose the only sensible option: **8%**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bayes Rule in Cancer Test Case\n",
    "\n",
    "Bayes Rule allows us to update our prior beliefs based on new evidence. In the case of the cancer test, we're updating our prior probabilities of having cancer with the test evidence to get our posterior probabilities.\n",
    "\n",
    "The probabilities are defined as follows:\n",
    "\n",
    "- **Prior Probabilities**\n",
    "- **P(Cancer) = 0.01**: The probability that a person has cancer.\n",
    "- **P(¬Cancer) = 0.99**: The probability that a person doesn't have cancer.\n",
    "\n",
    "- **Test Evidence**\n",
    "- **P(Positive|Cancer) = 0.9**: The probability that a person with cancer tests positive.\n",
    "- **P(Negative|¬Cancer) = 0.9**: The probability that a person without cancer tests negative.\n",
    "- **P(Positive|¬Cancer) = 0.1**: The probability that a person without cancer tests positive.\n",
    "\n",
    "- **Posterior Probabilities** (to be calculated)\n",
    "- **P(Cancer|Positive) = ?**: The probability that a person really has cancer given a positive test result.\n",
    "- **P(¬Cancer|Positive) = ?**: The probability that a person really doesn't have cancer given a positive test result.\n",
    "\n",
    "We use Bayes Rule to update our prior probabilities based on the test evidence:\n",
    "\n",
    "> P(Cancer|Positive) = P(Cancer) * P(Positive|Cancer)\n",
    "\n",
    "> P(¬Cancer|Positive) = P(¬Cancer) * P(Positive|¬Cancer)\n",
    "\n",
    "After calculating these, we get:\n",
    "\n",
    "> P(Cancer|Positive) = 0.01 * 0.90 = 0.009\n",
    "\n",
    "> P(¬Cancer|Positive) = 0.99 * 0.1 = 0.099\n",
    "\n",
    "This gives us our updated probabilities after considering the test results.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Bayes](./images/6.png)\n",
    "\n",
    "# Normalizing Posterior Probabilities\n",
    "\n",
    "Once we have our posterior probabilities, we must ensure that they form a valid probability distribution. That is, their sum must be equal to 1. If not, we need to normalize them.\n",
    "\n",
    "The posterior probabilities we calculated earlier were:\n",
    "\n",
    "- **P(Cancer|Positive) = 0.009**\n",
    "- **P(¬Cancer|Positive) = 0.099**\n",
    "\n",
    "The sum of these probabilities is our normalizer:\n",
    "\n",
    "> Normalizer = P(Cancer|Positive) + P(¬Cancer|Positive) = 0.009 + 0.099 = 0.108\n",
    "\n",
    "This means that there is a 0.108 probability of a positive test result.\n",
    "\n",
    "But, as these probabilities do not add up to 1, we must normalize them to get a valid probability distribution. This is done by dividing each probability by the sum of the probabilities (the normalizer).\n",
    "\n",
    "Next step will be to calculate the normalized probabilities.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Bayes](./images/7.png)\n",
    "\n",
    "# Total Probability and Normalization\n",
    "\n",
    "Here, we have our joint probabilities and our normalizer:\n",
    "\n",
    "- **P(Cancer, Positive) = 0.009**\n",
    "- **P(¬Cancer, Positive) = 0.099**\n",
    "- **Normalizer = 0.108**\n",
    "\n",
    "We can now normalize these probabilities by dividing each of them by the normalizer. This process gives us the total probability.\n",
    "\n",
    "- **P(Cancer|Positive) = P(Cancer, Positive) / Normalizer = 0.009 / 0.108 = 0.0833**\n",
    "- **P(¬Cancer|Positive) = P(¬Cancer, Positive) / Normalizer = 0.099 / 0.108 = 0.9167**\n",
    "\n",
    "We can verify our calculations by checking that these probabilities add up to 1.\n",
    "\n",
    "- **Total Probability = P(Cancer|Positive) + P(¬Cancer|Positive) = 0.0833 + 0.9167 = 1**\n",
    "\n",
    "We have successfully normalized our probabilities and calculated the total probability.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bayes Rule Diagram\n",
    "\n",
    "Bayes Rule can be thought of as a process with a sequence of steps.\n",
    "\n",
    "Let's say we get a positive test and we want to look at both the cancer and no cancer hypotheses:\n",
    "\n",
    "1. **Cancer Hypothesis** = Prior probability (P(Cancer)) x Sensitivity (P(Positive|Cancer))\n",
    "2. **No Cancer Hypothesis** = Prior probability (P(¬Cancer)) x (1 - Specificity (P(Negative|¬Cancer)))\n",
    "\n",
    "The sum of these two numbers is the **Normalizer** (Cancer Hypothesis + No Cancer Hypothesis), and it will usually not be 1.\n",
    "\n",
    "The next step in the process is to normalize the hypotheses using the Normalizer. This Normalizer represents the probability of a positive test, and is independent of the cancer diagnosis. Therefore, it can be used to normalize both cases:\n",
    "\n",
    "3. **Posterior Probability (Cancer)** = Cancer Hypothesis / Normalizer\n",
    "4. **Posterior Probability (No Cancer)** = No Cancer Hypothesis / Normalizer\n",
    "\n",
    "Finally, we verify our calculations:\n",
    "\n",
    "5. The sum of the posterior probabilities should be 1.\n",
    "\n",
    "This is the algorithm for Bayes Rule.\n",
    "\n",
    "![Bayes_Rule](./images/8.png)\n",
    "\n",
    "# For Negative\n",
    "\n",
    "![Bayes_Rule2](./images/9.png)\n",
    "\n",
    "# Equivalent Diagram for Negative Test Results\n",
    "\n",
    "The Bayes Rule algorithm works similarly when examining probabilities for negative test results.\n",
    "\n",
    "Let's say we now have a negative test, and we want to look at both the cancer and no cancer hypotheses:\n",
    "\n",
    "1. **Cancer Hypothesis** = Prior probability (P(Cancer)) x (1 - Sensitivity (P(Positive|Cancer)))\n",
    "2. **No Cancer Hypothesis** = Prior probability (P(¬Cancer)) x Specificity (P(Negative|¬Cancer))\n",
    "\n",
    "The sum of these two numbers is the **Normalizer** (Cancer Hypothesis + No Cancer Hypothesis), and it will usually not be 1.\n",
    "\n",
    "The next step in the process is to normalize the hypotheses using the Normalizer. This Normalizer represents the probability of a negative test, and is independent of the cancer diagnosis. Therefore, it can be used to normalize both cases:\n",
    "\n",
    "3. **Posterior Probability (Cancer)** = Cancer Hypothesis / Normalizer\n",
    "4. **Posterior Probability (No Cancer)** = No Cancer Hypothesis / Normalizer\n",
    "\n",
    "Finally, we verify our calculations:\n",
    "\n",
    "5. The sum of the posterior probabilities should be 1.\n",
    "\n",
    "These steps could also be stated in terms of joint probabilities:\n",
    "\n",
    "- P(Pos, C) = P(Pos|C) P(C)\n",
    "- P(Neg, C) = P(Neg|C) P(C)\n",
    "- P(Pos, ¬C) = P(Pos|¬C) P(C)\n",
    "- P(Neg, ¬C) = P(Neg|¬C) P(C)\n",
    "\n",
    "# Lesson Overview\n",
    "\n",
    "Congratulations on completing this lesson on Bayes Rules and probabilistic inference! In this lesson, you have accomplished the following:\n",
    "\n",
    "1. Evaluated cases with hidden variables\n",
    "2. Constructed probabilities for joint events\n",
    "3. Determined posterior probabilities using normalizers\n",
    "\n",
    "You have also learned how to apply **Bayes Rule** for various use cases, both large and small.\n",
    "\n",
    "## Additional Resources:\n",
    "\n",
    "- A [Blog about Bayes Rule in Everyday Life](https://blogs.cornell.edu/info2040/2018/11/19/bayes-theorem-application-in-everyday-life/) helps provide context for understanding how this rule applies to common situations.\n",
    "\n",
    "- [A Practical Example of Bayes Rule](https://towardsdatascience.com/bayes-rule-with-a-simple-and-practical-example-2bce3d0f4ad0): This example on Towards Data Science helps solidify understanding of Bayes Rule with a tangible example.\n",
    "\n",
    "_Note: Links provided above may require internet connection to access. Please copy and paste the URLs in your web browser to view the resources._\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Online store scenario\n",
    "\n",
    "Suppose you have an online store and want to add a new feature: a quick view overlay. Clicking on the product image brings up an overlay summarizing product info without taking users away from the search results. You'd like to test if this new feature leads to more sales.\n",
    "\n",
    "Running an experiment would allow us to ascertain whether or not the overlay caused a change in customers' purchasing habits. But what makes an experiment an experiment?\n",
    "\n",
    "1. We need to perform a comparison between two groups of users. Comparing the number of purchases made by users interacting with the old website (without the overlay) to the number of purchases made by users interacting with the new website (with the overlay) allows us to discern the effect of the overlay.\n",
    "2. We need to control for other differences between the groups. The only difference between the two groups should be whether the customer gets the site with the overlay or the site without the overlay. Random assignment of individuals to each site version is a common method for ensuring this.\n",
    "\n",
    "## Types of Study\n",
    "\n",
    "There are many ways to collect data to test or understand the relationship between two variables of interest. These methods fall into three main categories, based on the degree of control that you hold over the variables in play:\n",
    "\n",
    "1. If you have a lot of control over the features, then you have an **experiment**.\n",
    "2. If you have no control over the features, then you have an **observational study**.\n",
    "3. If you have some control, then you have a **quasi-experiment**.\n",
    "\n",
    "Understanding these types of studies can be very useful, especially if an experiment cannot be run.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "In social and medical sciences, an experiment is defined by comparing outcomes between two or more groups, ensuring equivalence between the compared groups except for the manipulation we want to test. The goal is to see if a change in one feature has an effect on the value of a second feature.\n",
    "\n",
    "Equivalence between groups is typically carried out through some kind of randomization procedure. If we randomly assign our units of analysis to each group, then we should expect the feature distributions between groups to be about the same. This theoretically isolates the changes in the outcome to the changes in our manipulated feature.\n",
    "\n",
    "## Observational Studies\n",
    "\n",
    "Unlike experiments where we exert a lot of control, observational studies are defined by a lack of control. In these studies, no control is exerted on the variables of interest, perhaps due to ethical concerns or a lack of power to enact the manipulation.\n",
    "\n",
    "We typically cannot infer causality in an observational study due to our lack of control over the variables. However, this does not mean that they are not useful. Observational studies can provide an interesting relationship needed to perform additional studies or to collect more data.\n",
    "\n",
    "## Quasi-Experiments\n",
    "\n",
    "Quasi-experiments lie between the observational study and the experiment. In these studies, some, but not all, of the control requirements of a true experiment, are met.\n",
    "\n",
    "While a quasi-experiment may not have the same strength of causality inference as a true experiment, the results can still provide a strong amount of evidence for the relationship being investigated.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Study Description | Type of Study\n",
    "--- | ---\n",
    "A blogger tries out a celebrity diet and exercise regimen and documents their changes in weight and energy. | Quasi-Experiment\n",
    "An online retailer sends out personal coupon codes to half of their mailing list. | Experiment\n",
    "Historic data is collected to compare the effect of local curfew ordinances and downtown foot traffic. | Observational Study\n",
    "A regional store chain tests out a promotional discount at some of their stores to see if it increases sales. | Quasi-Experiment\n",
    "Researchers compare the race times for runners based on the type of shoe that they wear. | Observational Study\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "n = 20\n",
    "p = 0.5\n",
    "\n",
    "dist = binom(n, p)\n",
    "values = range(n+1)\n",
    "probabilities = [dist.pmf(k) for k in values]\n",
    "\n",
    "plt.bar(values, probabilities)\n",
    "plt.title(\"Binomial Distribution\")\n",
    "plt.xlabel(\"Number of Heads (k)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T13:30:20.738390100Z",
     "start_time": "2023-07-26T13:30:09.803945100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Objective\n",
    "\n",
    "We have a fair coin (P = 0.5), and we flip it 20 times (N = 20). Our task is to find the number of heads (k) that maximizes the probability in the binomial distribution.\n",
    "\n",
    "# Binomial Distribution\n",
    "\n",
    "The binomial distribution is used when there are exactly two mutually exclusive outcomes of a trial, often referred to as \"success\" and \"failure\". The parameters of a binomial distribution are `n` and `p` where `n` is the total number of trials, and `p` is the probability of success in a given trial.\n",
    "\n",
    "In our case, flipping a coin is a binomial experiment, where getting a head is a success and getting a tail is a failure (or vice versa). We have `n=20` trials (coin flips), and the probability of getting a head (`p`) is 0.5 (since the coin is fair).\n",
    "\n",
    "# Probability Mass Function (PMF)\n",
    "\n",
    "The PMF equation gives the probability of obtaining exactly `k` successes in `n` trials:\n",
    "\n",
    "P(k) = nCk * p^k * (1−p)^(n−k)\n",
    "\n",
    "where,\n",
    "- nCk = n! / [(n-k)!k!] is the number of combinations of n items taken k at a time.\n",
    "\n",
    "The Probability Mass Function (PMF) for a binomial distribution is defined as:\n",
    "\n",
    "$P(X=k) = \\binom{n}{k} \\cdot p^k \\cdot (1−p)^{n−k}$\n",
    "\n",
    "\n",
    "# Analysis\n",
    "\n",
    "We calculate P(k) for each k from 0 to 20 and plot the values. In the plot, the x-axis represents the number of heads (k), and the y-axis represents the corresponding probability.\n",
    "\n",
    "# Result\n",
    "\n",
    "Upon plotting, we observe that k = 10 gives us the highest probability, which means that when flipping a fair coin 20 times, the most likely outcome is getting exactly 10 heads.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Normal Distribution (Bell Curve)\n",
    "\n",
    "The \"bell curve\" is often referred to as a normal distribution. A normal distribution is characterized by two parameters: the mean (μ) and the variance (σ²).\n",
    "\n",
    "If we consider any outcome (x), the squared difference between the value (x) and the mean (μ) can be expressed as:\n",
    "\n",
    "$(x−μ)^2$\n",
    "\n",
    "This represents a function of quadratic difference.\n",
    "\n",
    "_What does this function of quadratic difference look like?_\n",
    "\n",
    "![Graph](./images/10.png)\n",
    "\n",
    "Analyzing the properties of the quadratic function helps us understand the shape it takes:\n",
    "\n",
    "1. It's a quadratic function, which means it's a parabola. This immediately rules out any linear representations.\n",
    "2. When $x=μ$, $f(x)=0$. This property is a hallmark of a quadratic function with its minimum at x=μ.\n",
    "3. The function is always non-negative (i.e., it's always above the x-axis) due to the square term. This ensures the function doesn't dip below the x-axis.\n",
    "\n",
    "Considering all these properties, we find that the function of quadratic difference will resemble the shape of the upper-right graph in your image. This is the typical \"bell curve\" shape we associate with the normal distribution.\n",
    "\n",
    "# Impact of Variance on the Distribution\n",
    "\n",
    "Let's examine the following function:\n",
    "\n",
    "$\\frac{(x−μ)^2}{σ^2}$\n",
    "\n",
    "Here, $σ^2$ represents the variance.\n",
    "\n",
    "Now consider the shape of this function when $σ^2=1$. What would the curve's shape be when $σ^2=4$? Would it be wider, narrower, or the same?\n",
    "\n",
    "![Graph2](./images/11.png)\n",
    "\n",
    "The variance, $σ^2$, impacts the distribution along the y-axis. Since it is in the denominator, a larger value (like $σ^2=4$) will \"compress\" the function vertically, causing the distribution to widen. On the other hand, a smaller variance value would \"stretch\" the function vertically, leading to a narrower distribution.\n",
    "\n",
    "Hence, when $σ^2$ increases from 1 to 4, the shape of the distribution will become **wider**.\n",
    "\n",
    "# Variance and Curve Width\n",
    "\n",
    "Let's think about the quadratic function in a different way. Consider a curve that is narrower than the one when $σ^2=1$. What could be the possible value of $σ^2$ for this curve?\n",
    "\n",
    "Given what we have learned so far:\n",
    "\n",
    "- $σ^2=4$ results in a wider curve.\n",
    "- $σ^2=1$ results in a curve of the same width.\n",
    "- $σ^2=0$ results in a spike (a curve with no width).\n",
    "  \n",
    "Thus, only a value of $σ^2=1/4$ would create a **narrower** curve compared to when $σ^2=1$.\n",
    "\n",
    "# Exponential of Quadratic Function\n",
    "\n",
    "We consider the exponential of the quadratic function, which gives us:\n",
    "\n",
    "$f(x)=e^{-\\frac{1}{2} \\left( \\frac{x−μ}{σ} \\right)^2}$\n",
    "\n",
    "To solve the exercise without plugging in actual numbers, it's crucial to understand the curve of the exponential function $f(x)=e^x$.\n",
    "\n",
    "As depicted in the following graph, the value of the function is always positive and equals 1 when $x=0$. Importantly, the function is monotonic, meaning a larger $x$ results in a larger function value.\n",
    "\n",
    "![Graph3](./images/12.png)\n",
    "\n",
    "To figure out the maximum value of the given exponential function, we must determine the maximum value of $−\\frac{1}{2} \\left( \\frac{x−μ}{σ} \\right)^2$.\n",
    "\n",
    "Given that $−\\frac{1}{2} \\left( \\frac{x−μ}{σ} \\right)^2$ is less than or equal to 0, its maximum is 0, which occurs when $x=μ$. Consequently, the exponential function reaches its maximum when $x=μ$.\n",
    "\n",
    "# Value of the Exponential Function at Mean\n",
    "\n",
    "We consider the function:\n",
    "\n",
    "$f(x)=e^{-\\frac{1}{2} \\left( \\frac{x−μ}{σ} \\right)^2}$\n",
    "\n",
    "We ask: what is the value of $f(μ)$?\n",
    "\n",
    "With $x=μ$, $−\\frac{1}{2} \\left( \\frac{x−μ}{σ} \\right)^2 = 0$.\n",
    "\n",
    "Given that $e^0 = 1$, the answer is 1.\n",
    "\n",
    "# Minimal Value of the Exponential Function \n",
    "\n",
    "When we look again at the curve of the exponential function, it's clear that the function is monotonic, and a smaller $x$ results in a smaller value of the function. \n",
    "\n",
    "So the question essentially asks us what value of $x$ minimizes $-\\frac{1}{2}\\left( \\frac{x−μ}{σ} \\right)^2$.\n",
    "\n",
    "![graph](./images/13.png)\n",
    "\n",
    "The shape of $-\\frac{1}{2}\\left( \\frac{x−μ}{σ} \\right)^2$ is shown below, and we can see that when $x$ approaches $+\\infty$ and $-\\infty$, the value is minimized, and thus so is the value of the exponential function.\n",
    "\n",
    "![graph](./images/14.png)\n",
    "\n",
    "# Bell Curve and the Normal Distribution\n",
    "\n",
    "Given the function $f(x)=e^{-\\frac{1}{2}\\left( \\frac{x−μ}{σ} \\right)^2}$, we know that:\n",
    "\n",
    "- The maximum value is 1 when $x=μ$\n",
    "- The minimum value is 0 when $x=±∞$\n",
    "\n",
    "Thus, we can draw the curve of the function as shown below.\n",
    "\n",
    "![graph](./images/15.png)\n",
    "\n",
    "This plot represents the bell curve. The maximum of the curve is 1 when $x = μ$.\n",
    "\n",
    "The distribution described here is what you get when computing a mean over any set of experiments. Regardless of the nature of the experiment, as long as you do it a large number of times, the mean of the results will always have a distribution like this.\n",
    "\n",
    "However, there is a limitation to this distribution - the area under the curve sums up to $2πσ^2$, which is not particularly useful for studying statistics and probability because probabilities always add up to 1. Therefore, the true normal distribution is normalized by $1/ \\sqrt{2πσ^2}$.\n",
    "\n",
    "The normal distribution can thus be expressed as:\n",
    "\n",
    "$$\n",
    "N(x;μ,σ^2)= \\frac{1}{\\sqrt{2πσ^2}} ∗ e^{-\\frac{1}{2}\\left( \\frac{x−μ}{σ} \\right)^2}\n",
    "$$\n",
    "\n",
    "where $μ$ is the mean and $σ^2$ is the variance of the normal distribution.\n",
    "\n",
    "# Normal Distribution\n",
    "\n",
    "The normal distribution can be represented as:\n",
    "\n",
    "$$\n",
    "N(x;μ,σ^2)= \\frac{1}{\\sqrt{2πσ^2}} ∗ e^{-\\frac{1}{2}\\left( \\frac{x−μ}{σ} \\right)^2}\n",
    "$$\n",
    "\n",
    "where $μ$ is the mean and $σ^2$ is the variance of the normal distribution. The quadratic term penalizes any deviations from the expected value $μ$, and the exponential term shapes the curve.\n",
    "\n",
    "The shape of the normal distribution is shown below. The y-axis represents the probability $p(x)$ and x represents the possible mean values of the experiments. We can infer some information from the curve:\n",
    "\n",
    "- The probability is maximized when $x=μ$\n",
    "- The probability decreases when $x$ deviates from $μ$\n",
    "- The probability approaches 0 when $x$ is significantly different from the mean.\n",
    "\n",
    "![graph](./images/16.png)\n",
    "\n",
    "# Further Reading\n",
    "\n",
    "- If you want to read more about the property of the normal distribution, you can look at [this article](https://www.statisticshowto.com/probability-and-statistics/normal-distributions/).\n",
    "- If you want to dive deep into the mathematics behind the normal distribution, you can read the [wiki page](https://en.wikipedia.org/wiki/Normal_distribution) and [this article](https://mathworld.wolfram.com/NormalDistribution.html).\n",
    "\n",
    "\n",
    "# Central Limit Theorem and the Normal Distribution\n",
    "\n",
    "If you are conducting an experiment and your observation of a single experiment is a sample, the Central Limit Theorem plays a key role. The theorem tells us that if we conduct this experiment repeatedly and independently and obtain a sufficiently large number of samples, the distribution of the sample means will approximate a normal distribution. This holds true even if the original observation of the experiment is not normally distributed.\n",
    "\n",
    "The significance of the normal distribution in statistics is that it can be applied to problems involving other types of distributions if the sample size is large enough.\n",
    "\n",
    "Let's revisit the coin flip example to understand this better:\n",
    "\n",
    "- If you only flip a coin once, you use a single probability to describe the outcome of getting Heads.\n",
    "- If you flip a coin a few times, you can use the binomial distribution to describe the probability of getting a given number of Heads.\n",
    "- If you flip a coin 10,000 times, the probability of getting a given number of Heads is approximately a normal distribution.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inferential Statistics\n",
    "\n",
    "This section aims to ensure your comfort with the ideas surrounding inferential statistics. The next four concepts will highlight the difference between Descriptive and Inferential Statistics. Application of inferential statistics techniques to data will be covered in subsequent lessons.\n",
    "\n",
    "## Probability to Statistics\n",
    "\n",
    "This marks the beginning of a series of lessons that will be more data-oriented in applying ideas, and less probability-oriented.\n",
    "\n",
    "Often, we use statistics to validate conclusions of probability through simulation. Therefore, simulation (similar to the Python lesson you completed earlier) will be a substantial part of how we demonstrate mathematical ideas moving forward.\n",
    "\n",
    "# Descriptive Statistics\n",
    "\n",
    "Descriptive statistics involves describing our collected data using the measures we've discussed: measures of center, measures of spread, the shape of our distribution, and outliers. We can also use plots of our data to gain a better understanding.\n",
    "\n",
    "# Inferential Statistics\n",
    "\n",
    "Inferential Statistics is about using our collected data to draw conclusions about a larger population. To do inferential statistics well, it requires that we take a sample that accurately represents our population of interest.\n",
    "\n",
    "Surveys are a common way to collect data. However, they can be heavily biased depending on the types of questions that are asked and how those questions are asked. This is a topic you should think about when undertaking a project.\n",
    "\n",
    "In this lesson, we looked at specific examples that allowed us to identify:\n",
    "\n",
    "- **Population** - our entire group of interest.\n",
    "- **Parameter** - a numerical summary about a population.\n",
    "- **Sample** - a subset of the population.\n",
    "- **Statistic** - a numerical summary about a sample.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sampling Distributions\n",
    "\n",
    "A sampling distribution is the distribution of a statistic. In this case, we're looking at the distribution of proportions for samples of 5 students. This concept is key to the ideas we've covered in this lesson and will cover in future lessons.\n",
    "\n",
    "# Introducing NumPy\n",
    "\n",
    "NumPy is a fantastic tool for mathematical operations in Python. It has numerous built-in functionalities and performs computations faster than Python alone since it's implemented in C.\n",
    "\n",
    "While NumPy is a large library, we will only scratch the surface here. If you plan on doing heavy math in Python, you should take some time to explore its documentation.\n",
    "'\n",
    "# Importing NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Setting a random seed\n",
    "\n",
    "To get consistent results each time you run through a notebook, you can use `np.random.seed`. The number you pass in is not important, what matters is that you use the same one each time. Otherwise, if you simulate data with NumPy, you'll get different results based on the exact time when the code was executed.\n",
    "'\n",
    "# Setting a random seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# NumPy arrays\n",
    "\n",
    "To create an array, you pass a Python list to the `array` function.\n",
    "'\n",
    "# Creating a NumPy array\n",
    "v = np.array([1,2,3])\n",
    "\n",
    "# Finding summary statistics\n",
    "\n",
    "We can find the mean, variance, and standard deviation using NumPy functions.\n",
    "'\n",
    "# Calculating mean\n",
    "mean = np.mean(v)\n",
    "\n",
    "# Calculating variance\n",
    "variance = np.var(v)\n",
    "\n",
    "# Calculating standard deviation\n",
    "std_dev = np.std(v)\n",
    "\n",
    "mean, variance, std_dev\n",
    "\n",
    "# Simulating draws from arrays\n",
    "\n",
    "We can use NumPy's `random.choice` to simulate draws from the array. First, you pass in the array to be drawn from, then the number of times a choice should be made using the `size` parameter.\n",
    "'\n",
    "# Simulating draws from the array\n",
    "simulated_draws = np.random.choice(v, size=2)\n",
    "\n",
    "simulated_draws\n",
    "\n",
    "# Introducing Matplotlib\n",
    "\n",
    "Data visualization is a significant area within data analysis and data science. Matplotlib is one of the most popular Python libraries for data visualization. We will primarily use Matplotlib to create histograms for visual insight into our distributions.\n",
    "'\n",
    "# Importing Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting a histogram\n",
    "\n",
    "The simplest way to plot a histogram with Matplotlib is to call the `hist` function.\n",
    "'\n",
    "# Plotting a histogram\n",
    "plt.hist(v)\n",
    "plt.show()\n",
    "\n",
    "### Sampling Distributions Introduction\n",
    "\n",
    "In order to gain a bit more comfort with this idea of sampling distributions, let's do some practice in python.\n",
    "\n",
    "Below is an array that represents the students we saw in the previous videos, where 1 represents the students that drink coffee, and 0 represents the students that do not drink coffee.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original proportion of coffee drinkers: 0.7142857142857143\n",
      "Variance of original data: 0.20408163265306126\n",
      "Standard deviation of original data: 0.45175395145262565\n",
      "Mean of 10,000 sample proportions: 0.71396\n",
      "Variance of 10,000 sample proportions: 0.0417571184\n",
      "Standard deviation of 10,000 sample proportions: 0.2043455857120481\n",
      "p*(1-p): 0.20408163265306123\n",
      "sqrt(p*(1-p)/n): 0.20203050891044216\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create the students array\n",
    "students = np.array([1,0,1,1,1,1,0,0,0,0,1,1,1,1,1,1,1,1,1,1,0])\n",
    "\n",
    "# Compute the original proportion of coffee drinkers\n",
    "p = np.mean(students)\n",
    "print(f\"Original proportion of coffee drinkers: {p}\")\n",
    "\n",
    "# Calculate variance and standard deviation for the original 21 data values\n",
    "var_students = np.var(students)\n",
    "std_students = np.std(students)\n",
    "print(f\"Variance of original data: {var_students}\")\n",
    "print(f\"Standard deviation of original data: {std_students}\")\n",
    "\n",
    "# Obtain 10,000 additional proportions with samples of size 5\n",
    "sample_props = []\n",
    "for _ in range(10000):\n",
    "    sample = np.random.choice(students, 5, replace=True)\n",
    "    sample_props.append(np.mean(sample))\n",
    "\n",
    "# Compute the mean of these 10,000 proportions\n",
    "mean_sample_props = np.mean(sample_props)\n",
    "print(f\"Mean of 10,000 sample proportions: {mean_sample_props}\")\n",
    "\n",
    "# Calculate variance and standard deviation for the 10,000 proportions\n",
    "var_sample_props = np.var(sample_props)\n",
    "std_sample_props = np.std(sample_props)\n",
    "print(f\"Variance of 10,000 sample proportions: {var_sample_props}\")\n",
    "print(f\"Standard deviation of 10,000 sample proportions: {std_sample_props}\")\n",
    "\n",
    "# Compute p(1-p) and p(1-p)/n\n",
    "binom_var = p * (1-p)\n",
    "binom_std = np.sqrt(p * (1-p) / 5)  # n=5 for the sample size\n",
    "print(f\"p*(1-p): {binom_var}\")\n",
    "print(f\"sqrt(p*(1-p)/n): {binom_std}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T08:45:15.851797600Z",
     "start_time": "2023-08-01T08:45:14.563227500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exploring Other Sampling Distributions\n",
    "\n",
    "Sampling distributions provide insights into how a statistic can vary based on different samples drawn from the same population.\n",
    "\n",
    "While we've discussed sampling distributions in the context of population proportions (such as the proportion of coffee drinkers), they are not limited to proportions. We can generate sampling distributions for other statistical measures, too. For instance:\n",
    "\n",
    "- **Standard Deviation**: The extent to which data points deviate from the mean.\n",
    "- **Variance**: The square of the standard deviation, giving more weight to extreme deviations.\n",
    "- **Difference in Means**: The difference between the means of two groups or categories in our data.\n",
    "\n",
    "Remember, however, that these sampling distributions are built around 'statistics' (measures we calculate from our samples), not 'parameters' (measures from the whole population). Parameters, being population-wide measures, are fixed values and don't change with different samples. In contrast, statistics can change based on the specific sample you're examining, which is precisely what makes sampling distributions so interesting and informative.\n",
    "\n",
    "# Key Theorems for Sampling Distributions\n",
    "\n",
    "Sampling distributions work in tandem with two essential mathematical theorems:\n",
    "\n",
    "1. **Law of Large Numbers**: This law states that as our sample size increases, our statistic (like the mean or proportion) gets closer to the actual population parameter. In other words, larger samples tend to provide more accurate representations of the population.\n",
    "\n",
    "2. **Central Limit Theorem**: This theorem states that with a sufficiently large sample size, the sampling distribution of the mean will approximate a normal distribution, regardless of the shape of the population distribution. \n",
    "\n",
    "Notably, the Central Limit Theorem applies to various statistics:\n",
    "\n",
    "- Sample means (x̄)\n",
    "- Sample proportions (p)\n",
    "- Difference in sample means (x̄₁−x̄₂)\n",
    "- Difference in sample proportions (p₁−p₂)\n",
    "\n",
    "However, it doesn't apply to all statistics. We will explore this further later in this lesson.\n",
    "\n",
    "# Advanced Estimation Techniques\n",
    "\n",
    "If you're interested in deepening your understanding of estimation techniques beyond the course's scope, consider exploring these methodologies:\n",
    "\n",
    "- [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)\n",
    "- [Method of Moments Estimation](https://en.wikipedia.org/wiki/Method_of_moments_(statistics))\n",
    "- [Bayesian Estimation](https://en.wikipedia.org/wiki/Bayesian_inference)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of sample means: 100.86128281310164\n",
      "Standard deviation of sample means: 57.72519536610306\n",
      "Mean of population: 100.35978700795846\n",
      "Standard deviation of population: 99.77860187968906\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1200x600 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate population data\n",
    "pop_data = np.random.gamma(1,100,3000)\n",
    "\n",
    "# Draw sample means\n",
    "means_size_3 = [np.mean(np.random.choice(pop_data, 3)) for _ in range(10000)]\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(pop_data)\n",
    "plt.title('Population Data')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(means_size_3)\n",
    "plt.title('Sample Means')\n",
    "\n",
    "# Print statistics\n",
    "for name, data in [('sample means', means_size_3), ('population', pop_data)]:\n",
    "    print(f'Mean of {name}: {np.mean(data)}')\n",
    "    print(f'Standard deviation of {name}: {np.std(data)}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T10:40:41.952647400Z",
     "start_time": "2023-08-01T10:40:37.081628100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115.89760480820178\n",
      "Variance of pop_data: 9955.76939306549\n",
      "Variance of means_size_100: 99.44037691417994\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "pop_data = np.random.gamma(1,100,3000)\n",
    "plt.hist(pop_data);\n",
    "\n",
    "m = np.random.choice(pop_data,size=100)\n",
    "print(m.mean())\n",
    "\n",
    "means_size_100=[]\n",
    "for _ in range(10000):\n",
    "    sample = np.random.choice(pop_data, 100)\n",
    "    means_size_100.append(np.mean(sample))\n",
    "\n",
    "plt.hist(means_size_100)\n",
    "\n",
    "# Compute and print variances\n",
    "variance_pop_data = np.var(pop_data)\n",
    "variance_means_size_100 = np.var(means_size_100)\n",
    "\n",
    "print(f\"Variance of pop_data: {variance_pop_data}\")\n",
    "print(f\"Variance of means_size_100: {variance_means_size_100}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-01T12:46:25.154689300Z",
     "start_time": "2023-08-01T12:46:22.463615900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Limitations of the Central Limit Theorem\n",
    "\n",
    "While the Central Limit Theorem (CLT) is a powerful tool, it doesn't apply universally. For example, in certain scenarios where the sample size is small (like in our example with 3 draws), the sampling distribution might not approximate a normal distribution even if we increase the sample size.\n",
    "\n",
    "Moreover, the CLT does not apply to all statistics. Some sampling distributions will never approximate a normal distribution, regardless of the sample size. Hence, understanding which statistics will follow normal distributions and which won't is crucial.\n",
    "\n",
    "In the age of powerful computing, the significance of the CLT might seem less apparent. However, its implications and applications continue to be an essential part of statistical inference.\n",
    "\n",
    "# Bootstrapping and Sampling Distributions\n",
    "\n",
    "## Bootstrapping\n",
    "Bootstrapping is a statistical method that involves sampling with replacement. The probability of any number in our set remains the same, regardless of how many times it has been chosen. It's akin to flipping a coin or rolling a dice, where each event is independent of the previous one.\n",
    "\n",
    "Bootstrapping allows us to estimate the sampling distribution of almost any statistic using random sampling methods. This technique is especially useful when we have limited data to estimate the population parameter. \n",
    "\n",
    "Notably, bootstrapping is employed in leading machine learning algorithms such as random forests and gradient boosting for making predictions. The concept of bootstrapping was proposed by [Bradley Efron](https://en.wikipedia.org/wiki/Bradley_Efron) in the 1970s, leveraging computational power to solve statistical problems. \n",
    "\n",
    "Bootstrapping is metaphorically compared to pulling oneself out of quicksand using their own bootstraps. Rather than having complete population data to understand parameters, bootstrapping allows us to perform repeated sampling.\n",
    "\n",
    "More details on why bootstrapping works as a technique for inference can be found [here](https://stats.stackexchange.com/questions/26088/explaining-to-laypeople-why-bootstrapping-works).\n",
    "\n",
    "## Importance of Sampling Distributions\n",
    "Sampling distributions play a vital role in inferential techniques such as confidence intervals and hypothesis testing. Many formulas and calculators are available to calculate the values for these techniques. However, these often mask their assumptions and potential biases. With sampling distributions and bootstrapping, we can avoid relying on these formulas and understand how to calculate these values ourselves, thereby minimizing bias.\n",
    "\n",
    "## Key Takeaways\n",
    "- Sampling distributions represent the distribution of a statistic.\n",
    "- The Law of Large Numbers and The Central Limit Theorem are two fundamental theorems related to sampling distributions.\n",
    "- The Law of Large Numbers states that as the sample size increases, the sample mean converges to the population mean.\n",
    "- The Central Limit Theorem states that the sample mean will follow a normal distribution for large sample sizes. However, this theorem extends beyond just the sample mean.\n",
    "- Bootstrapping is a technique of sampling with replacement, facilitating the simulation of sampling distributions.\n",
    "- Bootstrapping allows us to understand the sampling distribution of our statistics by calculating repeated values of our statistics.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Confidence Intervals and Sample Statistics\n",
    "\n",
    "Understanding the relationship between sample statistics and confidence intervals can be likened to fishing. Trying to estimate a parameter with a sample statistic is akin to fishing with a pole, while estimating with a confidence interval is like fishing with a net. The broader the net (i.e., the wider the confidence interval), the more likely you are to \"catch\" the true population parameter.\n",
    "\n",
    "## Transitioning from Sampling Distributions to Confidence Intervals\n",
    "\n",
    "We can leverage the concepts of bootstrapping and sampling distributions to construct confidence intervals for our parameters of interest.\n",
    "\n",
    "First, identify the statistic that best estimates your parameter(s) of interest. For instance, to estimate the population mean, we might use the sample mean. Alternatively, to estimate the difference in population means, we might use the difference in sample means.\n",
    "\n",
    "Once we have this statistic, we can construct confidence intervals for the parameter of interest. The confidence interval provides a range of plausible values for the population parameter, adding a margin of error to our point estimate, thereby giving us a better chance to \"catch\" the true parameter value.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Opening a CSV file\n",
    "\n",
    "df = pd.read_csv(\"filename.csv\")\n",
    "\n",
    "# Inspecting the dataframe\n",
    "\n",
    "**Shape**\n",
    "\n",
    "df.shape\n",
    "\n",
    "**Head**\n",
    "\n",
    "df.head()\n",
    "\n",
    "**Tail**\n",
    "\n",
    "df.tail()\n",
    "\n",
    "**Summary statistics**\n",
    "\n",
    "df.describe()\n",
    "\n",
    "df.mean()\n",
    "\n",
    "df.isnull()\n",
    "\n",
    "# Accessing data\n",
    "\n",
    "**Columns**\n",
    "\n",
    "df[['column_label']]  # double brackets returns a dataframe\n",
    "\n",
    "df[\"column_label\"]   # single brackets returns a series\n",
    "\n",
    "**Rows**\n",
    "\n",
    "df[\"column_label\"][\"row_label\"]  # returns a single value, not a series or dataframe\n",
    "\n",
    "df[df[\"column_label\"] > 5]  # returns a dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Applications of Confidence Intervals\n",
    "\n",
    "Confidence intervals have a wide range of applications, particularly in comparing the means of two groups, which is a common task in various fields:\n",
    "\n",
    "1. **Pharmaceutical Trials:** Confidence intervals can be used to determine whether a new drug has a statistically significant effect compared to a placebo or a current treatment option.\n",
    "\n",
    "2. **Education Research:** In education, they can help evaluate the effectiveness of a new teaching method or curriculum by comparing students' performances.\n",
    "\n",
    "3. **A/B Testing:** This technique is crucial in business analytics and data science. It involves changing an element on a webpage and using confidence intervals to compare the effects on user behavior, such as click-through rates, revenue, and customer satisfaction. A/B testing guides the decision-making process for optimal webpage design and user experience.\n",
    "\n",
    "# Statistical vs. Practical Significance\n",
    "\n",
    "Understanding the difference between statistical and practical significance is crucial in making data-informed decisions.\n",
    "\n",
    "- **Statistical Significance:** This is determined through hypothesis testing and confidence intervals. It indicates whether the observed effect in data occurred by chance or is a true effect.\n",
    "\n",
    "- **Practical Significance:** This refers to the real-world impact of the observed effect, considering other factors like resource constraints (time, money, manpower), business strategy, or real-world feasibility that might not be reflected directly in a statistical test.\n",
    "\n",
    "In decision-making, both statistical and practical significance should be considered. An effect can be statistically significant but not practically significant, and vice versa. Thus, it's crucial to weigh both when interpreting the results of data analysis.\n",
    "\n",
    "# The Future of Traditional Statistical Methods\n",
    "\n",
    "With the advent of modern computing capabilities, traditional statistical methods, including numerous hypothesis tests, are becoming less necessary. The future of statistics is leaning towards techniques like bootstrapping, which are flexible and less reliant on stringent assumptions.\n",
    "\n",
    "While we won't delve deep into traditional methods in this lesson, it's important to be aware of them. Many traditional hypothesis tests have corresponding confidence intervals, and these concepts still underpin much of statistical theory.\n",
    "\n",
    "For a deeper exploration into these traditional methods, visit the Stat Trek site. It provides extensive documentation on a variety of hypothesis tests. \n",
    "\n",
    "Remember, while understanding traditional methods is beneficial, the trend is to lean on modern, computationally-intensive methods like bootstrapping for statistical inference.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "coffee_full = pd.read_csv('coffee-dataset.csv')\n",
    "coffee_red = coffee_full.sample(200)\n",
    "\n",
    "#Bootstrap easy\n",
    "diff=[]\n",
    "for _ in range(10000):\n",
    "    bootsample = coffee_red.sample(200,replace=True)\n",
    "    mean_coff = bootsample[bootsample['drinks_coffee'] == True]['height'].mean()\n",
    "    mean_nocoff= bootsample[bootsample['drinks_coffee'] == False]['height'].mean()\n",
    "    diff.append(mean_coff-mean_nocoff)\n",
    "np.percentile(diff,2.5),np.percentile(diff,97.5)\n",
    "\n",
    "#Bootstrap Traditional\n",
    "\n",
    "import statsmodels.stats.api as sms\n",
    "x1= coffee_red[coffee_red['drinks_coffee'] == True]['height']\n",
    "x2= coffee_red[coffee_red['drinks_coffee'] == False]['height']\n",
    "\n",
    "cm=sms.CompareMeans(sms.DescrStatsW(x1),sms.DescrStatsW(x2))\n",
    "cm.tconfint_diff(usevar='unequal')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Traditional Methods vs. Bootstrap Confidence Intervals in Python\n",
    "\n",
    "Traditional methods for calculating a difference of means and bootstrap methods often yield similar results with large sample sizes. However, for small sample sizes, both methods can be problematic. Traditional methods might be based on unrealistic assumptions, while bootstrapping might produce misleading results due to insufficient representation of the entire population.\n",
    "\n",
    "## Variety of Confidence Intervals and Hypothesis Tests\n",
    "\n",
    "Traditional hypothesis tests include, but are not limited to:\n",
    "\n",
    "- T-Test\n",
    "- Two-Sample T-Test\n",
    "- Paired T-Test\n",
    "- Z-Test\n",
    "- Chi-Squared Test\n",
    "- F-Test\n",
    "\n",
    "Bootstrapping can be used as a flexible alternative to these techniques.\n",
    "\n",
    "For coding issues and inquiries, Stack Overflow and Google search are valuable resources. An example is this Stack Overflow page: [Confidence Interval for t-test (difference between means) in Python](https://stackoverflow.com/questions/31768464/confidence-interval-for-t-test-difference-between-means-in-python).\n",
    "\n",
    "However, note that the code from the linked Stack Overflow page has been deprecated and no longer works. The comparison table below shows that bootstrapping and built-in methods yield similar results.\n",
    "\n",
    "| Method        | Lower           | Upper  |\n",
    "| ------------- |:-------------:| -----:|\n",
    "| Bootstrapping | 0.3986866790986274 | 2.2432588681124224 |\n",
    "| Built-In | 0.39600106159185644 | 2.2734131570228908 |\n",
    "\n",
    "# Confidence Interval (CI) Width & Margin of Error\n",
    "\n",
    "When constructing confidence intervals, understanding the relationship between sample size and confidence level is crucial.\n",
    "\n",
    "Given all other factors remain constant:\n",
    "\n",
    "- **Increasing your sample size** will *decrease* the width of your confidence interval.\n",
    "- **Increasing your confidence level** (e.g., from 95% to 99%) will *increase* the width of your confidence interval.\n",
    "\n",
    "Additional points to remember:\n",
    "\n",
    "- The **confidence interval width** is the difference between the upper and lower bounds of your confidence interval.\n",
    "- The **margin of error** is half the confidence interval width, and is the value you add and subtract from your sample estimate to determine your confidence interval's final results.\n",
    "\n",
    "# What Confidence Intervals Can and Can't Tell Us\n",
    "\n",
    "Confidence intervals provide an aggregate perspective on data-based conclusions, primarily focusing on population parameters (aggregate population values).\n",
    "\n",
    "On the other hand, machine learning techniques adopt an individualistic approach to drawing conclusions as they attempt to predict an outcome for each specific data point.\n",
    "\n",
    "# Confidence Intervals: Key Takeaways\n",
    "\n",
    "In this lesson, we explored how to employ bootstrapping and knowledge of sampling distributions to establish confidence intervals for any population parameter. Key learnings include:\n",
    "\n",
    "1. **Confidence Interval Construction**: The process of building confidence intervals applies not only to the population mean and difference in means, but to any parameter of interest. \n",
    "\n",
    "2. **Python for Confidence Intervals**: Python built-in functions can be employed to build confidence intervals. However, it's important to note that these methods often rely on assumptions like the Central Limit Theorem. \n",
    "\n",
    "3. **Statistical vs Practical Significance**: We also examined the difference between statistical significance (evidence against a hypothesis) and practical significance (real-world importance). \n",
    "\n",
    "4. **Understanding Confidence Intervals**: Finally, we looked at terms associated with confidence intervals, such as 'margin of error' and 'confidence interval width'. Crucially, remember that confidence intervals are about parameters in a population, not about individual observations.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypothesis Testing: Key Takeaways\n",
    "\n",
    "Hypothesis testing is a systematic way to select samples from a group or population with the goal of making a determination about the expected behavior of the entire group. Some key points are:\n",
    "\n",
    "1. **Process**: Hypothesis tests typically start by formulating a null and alternative hypothesis, collecting and analyzing data, and then interpreting the results to see if the data provide enough evidence to reject the null hypothesis.\n",
    "\n",
    "2. **Types of Hypothesis Tests**: There are various kinds of hypothesis tests, including testing a population mean (one-sample t-test), testing the difference in means (two-sample t-test), testing the difference before and after some treatment on the same individual (paired t-test), testing a population proportion (one-sample z-test), and testing the difference between population proportions (two-sample z-test).\n",
    "\n",
    "3. **Resources**: Tables for t-distribution and z-distribution values, which are used in these tests, can be found online.\n",
    "\n",
    "4. **Bootstrapping and Hypothesis Testing**: Instead of memorizing how to perform all these different tests, bootstrapping provides a flexible workflow. You can find the statistic that best estimates the parameter you want to estimate, bootstrap to simulate the sampling distribution, and then use this distribution to aid in deciding on the hypothesis.\n",
    "\n",
    "5. **Hypothesis Tests for Population Parameters**: Always remember that hypothesis tests are performed on population parameters, not on statistics. Statistics are values you already have from the data, so performing hypothesis tests on these values is not meaningful.\n",
    "\n",
    "# Rules for Setting Up Null and Alternative Hypotheses\n",
    "\n",
    "Hypothesis testing often starts with defining a null hypothesis (H0) and an alternative hypothesis (H1). Here are some guidelines for setting up these hypotheses:\n",
    "\n",
    "1. **Null Hypothesis (H0)**: This hypothesis is assumed to be true before you collect any data. It usually suggests that there is no effect, or that two groups are equal. H0 always contains an equal sign of some kind - either =, ≤, or ≥.\n",
    "\n",
    "2. **Alternative Hypothesis (H1)**: This is what we aim to prove true. It is a competing, non-overlapping hypothesis to the null. The alternative hypothesis contains the opposition of the null - either ≠, >, or <.\n",
    "\n",
    "Consider the principle \"Innocent until proven guilty\". This idea can be translated into the following hypotheses:\n",
    "\n",
    "- H0: Innocent\n",
    "- H1: Guilty\n",
    "\n",
    "Before any evidence is collected, the null hypothesis assumes \"innocence\". The alternative hypothesis, which is competing and non-overlapping, proposes that the individual is \"guilty\".\n",
    "\n",
    "\n",
    "# Hypothesis Testing Errors: Type I and Type II\n",
    "\n",
    "In hypothesis testing, there are two types of errors we need to be aware of: Type I errors and Type II errors.\n",
    "\n",
    "## Null and Alternative Hypotheses\n",
    "\n",
    "When we set up hypotheses, the following rules apply:\n",
    "\n",
    "- The null hypothesis (H0) is assumed to be true before you collect any data.\n",
    "- H0 usually states there is no effect or that two groups are equal.\n",
    "- H0 and the alternative hypothesis (H1) are competing, non-overlapping hypotheses.\n",
    "- H1 is what we would like to prove to be true.\n",
    "- H0 contains an equal sign of some kind - either =, ≤, or ≥.\n",
    "- H1 contains the opposition of the null - either ≠, >, or <.\n",
    "\n",
    "For instance, when testing if a new page is better than an existing page:\n",
    "\n",
    "- H0: μ1 ≤ μ2 (the population mean return from the new page is less than or equal to that from the old page)\n",
    "- H1: μ1 > μ2 (the population mean return from the new page is greater than that from the old page)\n",
    "\n",
    "The hypotheses can be modified based on the question of interest.\n",
    "\n",
    "## Types of Errors\n",
    "\n",
    "In the context of a real-world judicial example, we can have these scenarios:\n",
    "\n",
    "|   | Truly Guilty | Truly Innocent |\n",
    "|---|---|---|\n",
    "| Guilty Decision | Correct decision | Type I error |\n",
    "| Innocent Decision | Type II error | Correct decision |\n",
    "\n",
    "Here, Type I error is sentencing an innocent person (a false positive), and Type II error is setting a guilty person free (a false negative).\n",
    "\n",
    "**Type I Errors**:\n",
    "- These should be set up such that they are the worse kind of errors.\n",
    "- Denoted by the symbol α or alpha.\n",
    "- Definition: Deciding the alternative (H1) is true, when actually, the null (H0) is true.\n",
    "- Often called false positives.\n",
    "\n",
    "**Type II Errors**:\n",
    "- Denoted by the symbol β or beta.\n",
    "- Definition: Deciding the null (H0) is true when actually, the alternative (H1) is true.\n",
    "- Often called false negatives.\n",
    "\n",
    "In extreme cases, we can always choose one hypothesis (like always choosing the null) to ensure a particular error never occurs. However, in general, a decrease in the chance of one type of error increases the chance of the other error occurring.\n",
    "\n",
    "# Hypothesis Testing: Type I and Type II Errors\n",
    "\n",
    "## Type I Errors\n",
    "\n",
    "Type I errors have the following features:\n",
    "\n",
    "- You should set up your null and alternative hypotheses so that the worst of your errors is the type I error.\n",
    "- They are denoted by the symbol α.\n",
    "- The definition of a type I error is: Deciding the alternative (H1) is true when actually, the null (H0) is true.\n",
    "- Type I errors are often called false positives.\n",
    "\n",
    "## Type II Errors\n",
    "\n",
    "Type II errors have the following features:\n",
    "\n",
    "- They are denoted by the symbol β.\n",
    "- The definition of a type II error is: Deciding the null (H0) is true when actually, the alternative (H1) is true.\n",
    "- Type II errors are often called false negatives.\n",
    "\n",
    "## Parachute Example\n",
    "\n",
    "This example lets you see one of the most extreme cases of errors that might be committed in hypothesis testing. In a type I error, an individual died. In a type II error, you lost 30 dollars.\n",
    "\n",
    "In the hypothesis tests you build in the upcoming lessons, you will be able to choose a type I error threshold, and your hypothesis tests will be created to minimize the type II errors after ensuring the type I error rate is met.\n",
    "\n",
    "## More Hypothesis Testing Practice\n",
    "\n",
    "Let's consider two hypothetical tests.\n",
    "\n",
    "1. You want to test if the average return of portfolio 1 (μ1) is greater than the average return of portfolio 2 (μ2). The hypotheses would be set up as:\n",
    "\n",
    "   - Null Hypothesis (H0): μ1 ≤ μ2\n",
    "   - Alternative Hypothesis (H1): μ1 > μ2\n",
    "\n",
    "2. You want to test if the average length of Chinook fish (μ1) is different than the average length of Coho fish (μ2). The hypotheses would be set up as:\n",
    "\n",
    "   - Null Hypothesis (H0): μ1 = μ2\n",
    "   - Alternative Hypothesis (H1): μ1 ≠ μ2\n",
    "\n",
    "# Hypothesis Selection\n",
    "\n",
    "Understanding how to choose between different hypotheses forms a crucial part of statistical analysis. This document demonstrates an example of how this process works, using the average height of individuals in a coffee dataset. Notably, the dataset includes both coffee drinkers and non-drinkers.\n",
    "\n",
    "## Confidence Interval Method\n",
    "\n",
    "One common approach to selecting a hypothesis involves the use of confidence intervals, an idea previously explored in the confidence interval lesson. By analyzing the confidence interval, you can determine if it falls within the null hypothesis space or the alternative hypothesis space.\n",
    "\n",
    "Consider the variable 'upper', corresponding to the 'high' variable in our video guide. This variable represents the upper limit of the confidence interval.\n",
    "\n",
    "For instance, if our interval was entirely below 70, we would choose the null hypothesis (stating that the population mean is less than 70). Hence, the placement of the interval can guide us towards the hypothesis we consider to be true.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In summary, hypothesis selection depends on a range of factors, including the results of statistical tests and the comparison of confidence intervals. Understanding these concepts allows you to make informed decisions when interpreting data and conducting research.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reject the null hypothesis\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here is the python code that we can use to simulate the example in the screencast. \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate a population mean\n",
    "pop_data = np.random.gamma(1,100,3000)\n",
    "plt.hist(pop_data);\n",
    "\n",
    "# Take a sample of 100 values\n",
    "sample = np.random.choice(pop_data, 100, replace=True)\n",
    "plt.hist(sample);\n",
    "\n",
    "# Compute the sample mean\n",
    "sample_mean = sample.mean()\n",
    "\n",
    "# Define the null and alternative hypotheses\n",
    "H0 = 70\n",
    "H1 = '!= 70'\n",
    "\n",
    "# Bootstrap our sample data\n",
    "bootsample = np.random.choice(sample, 100, replace=True)\n",
    "boot_means = []\n",
    "for _ in range(10000):\n",
    "    bootsample = np.random.choice(sample, 100, replace=True)\n",
    "    boot_means.append(bootsample.mean())\n",
    "\n",
    "# Compute the lower and upper bounds for a 95% confidence interval\n",
    "lower, upper = np.percentile(boot_means, 2.5), np.percentile(boot_means, 97.5)\n",
    "\n",
    "# Check if our observed statistic falls within the null hypothesis space or alternative hypothesis space\n",
    "if H0 >= lower and H0 <= upper:\n",
    "    print('Fail to reject the null hypothesis')\n",
    "else:\n",
    "    print('Reject the null hypothesis')\n",
    "\n",
    "# This prints: Fail to reject the null hypothesis\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T07:02:48.512282400Z",
     "start_time": "2023-08-03T07:02:43.987232500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simulating From the Null Hypothesis\n",
    "\n",
    "Load in the data below, and use the exercises to assist with answering the quiz questions below.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "\n",
    "full_data = pd.read_csv('coffee_dataset.csv')\n",
    "sample_data = full_data.sample(200)\n",
    "\n",
    "`1.` If you were interested in studying whether the average height for coffee drinkers is the same as for non-coffee drinkers, what would the null and alternative hypotheses be?  Write them in the cell below, and use your answer to answer the first quiz question below.\n",
    "\n",
    "**Since there is no directional component associated with this statement, a not equal to seems most reasonable.**\n",
    "\n",
    "$$H_0: \\mu_{coff} - \\mu_{no} = 0$$\n",
    "\n",
    "\n",
    "$$H_1: \\mu_{coff} - \\mu_{no} \\neq 0$$\n",
    "\n",
    "\n",
    "**$\\mu_{coff}$ and $\\mu_{no}$ are the population mean values for coffee drinkers and non-coffee drinkers, respectivley.**\n",
    "\n",
    "`2.` If you were interested in studying whether the average height for coffee drinkers is less than non-coffee drinkers, what would the null and alternative be?  Place them in the cell below, and use your answer to answer the second quiz question below.\n",
    "\n",
    "**In this case, there is a question associated with a direction - that is the average height for coffee drinkers is less than non-coffee drinkers.  Below is one of the ways you could write the null and alternative.  Since the mean for coffee drinkers is listed first here, the alternative would suggest that this is negative.**\n",
    "\n",
    "$$H_0: \\mu_{coff} - \\mu_{no} \\geq 0$$\n",
    "\n",
    "\n",
    "$$H_1: \\mu_{coff} - \\mu_{no} < 0$$\n",
    "\n",
    "\n",
    "**$\\mu_{coff}$ and $\\mu_{no}$ are the population mean values for coffee drinkers and non-coffee drinkers, respectivley.**\n",
    "\n",
    "`3.` For 10,000 iterations: bootstrap the sample data, calculate the mean height for coffee drinkers and non-coffee drinkers, and calculate the difference in means for each sample.  You will want to have three arrays at the end of the iterations - one for each mean and one for the difference in means.  Use the results of your sampling distribution, to answer the third quiz question below.\n",
    "\n",
    "nocoff_means, coff_means, diffs = [], [], []\n",
    "\n",
    "for _ in range(10000):\n",
    "    bootsamp = sample_data.sample(200, replace = True)\n",
    "    coff_mean = bootsamp[bootsamp['drinks_coffee'] == True]['height'].mean()\n",
    "    nocoff_mean = bootsamp[bootsamp['drinks_coffee'] == False]['height'].mean()\n",
    "    # append the info \n",
    "    coff_means.append(coff_mean)\n",
    "    nocoff_means.append(nocoff_mean)\n",
    "    diffs.append(coff_mean - nocoff_mean)\n",
    "\n",
    "np.std(nocoff_means) # the standard deviation of the sampling distribution for nocoff\n",
    "np.std(coff_means) # the standard deviation of the sampling distribution for coff\n",
    "np.std(diffs) # the standard deviation for the sampling distribution for difference in means\n",
    "plt.hist(nocoff_means, alpha = 0.5);\n",
    "plt.hist(coff_means, alpha = 0.5); # They look pretty normal to me!\n",
    "plt.hist(diffs, alpha = 0.5); # again normal - this is by the central limit theorem\n",
    "\n",
    "`4.` Now, use your sampling distribution for the difference in means and [the docs](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.normal.html) to simulate what you would expect if your sampling distribution were centered on zero.  Also, calculate the observed sample mean difference in `sample_data`. Use your solutions to answer the last questions in the quiz below.\n",
    "\n",
    "** We would expect the sampling distribution to be normal by the Central Limit Theorem, and we know the standard deviation of the sampling distribution of the difference in means from the previous question, so we can use this to simulate draws from the sampling distribution under the null hypothesis.  If there is truly no difference, then the difference between the means should be zero.**\n",
    "\n",
    "null_vals = np.random.normal(0, np.std(diffs), 10000) # Here are 10000 draws from the sampling distribution under the null\n",
    "plt.hist(null_vals); #Here is the sampling distribution of the difference under the null"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    # Understanding P-value\n",
    "\n",
    "The p-value is defined as the probability of observing your statistic, or one more extreme in favor of the alternative, assuming that the null hypothesis is true. This concept can be visually demonstrated using shading techniques in your statistical distribution.\n",
    "\n",
    "## Cases of P-value Calculation\n",
    "\n",
    "The direction and extent of shading, which correspond to the p-value, depends on the relationship of your observed statistic to the alternative hypothesis. Consider the following scenarios:\n",
    "\n",
    "1. **Parameter greater than a value:** If your parameter is greater than a specified value in the alternative hypothesis, your p-value is represented by a shaded region in your distribution as shown below:\n",
    "\n",
    "![Image Description](https://video.udacity-data.com/topher/2017/November/5a0230ff_screen-shot-2017-11-07-at-2.16.14-pm/screen-shot-2017-11-07-at-2.16.14-pm.png)\n",
    "\n",
    "2. **Parameter less than a value:** When your parameter is less than a certain value in the alternative hypothesis, the shading representing your p-value would look as follows:\n",
    "\n",
    "![Image Description2](https://video.udacity-data.com/topher/2017/November/5a023164_screen-shot-2017-11-07-at-2.18.27-pm/screen-shot-2017-11-07-at-2.18.27-pm.png)\n",
    "\n",
    "3. **Parameter not equal to a value:** If your parameter is not equal to a specified value in the alternative hypothesis, the shaded region denoting your p-value would appear like this:\n",
    "\n",
    "![Image Description3](https://video.udacity-data.com/topher/2017/November/5a02314d_screen-shot-2017-11-07-at-2.17.08-pm/screen-shot-2017-11-07-at-2.17.08-pm.png)\n",
    "\n",
    "You can calculate the p-value by integrating the sampling distribution to obtain the area for each of these shaded regions. Alternatively, these proportions can be simulated in upcoming lessons.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For not equal:\n",
    "\n",
    "# (nv<sm).mean() + (nv>nm+(nm-sm).mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypothesis Testing Procedure and Interpretation of P-values\n",
    "\n",
    "Hypothesis testing involves several key steps that help determine whether the observed data provides significant evidence against the null hypothesis.\n",
    "\n",
    "## Hypothesis Testing Steps:\n",
    "\n",
    "1. **Simulate the Null:** Simulate the possible values of your statistic under the null hypothesis.\n",
    "\n",
    "2. **Calculate Your Statistic:** Compute the actual value of the statistic from your data.\n",
    "\n",
    "3. **Compare to Null:** Evaluate your statistic against the values from the null distribution.\n",
    "\n",
    "4. **Evaluate Extremes:** Determine the proportion of null values that are considered extreme based on your alternative hypothesis.\n",
    "\n",
    "The resulting value is the **p-value**, which represents the probability of observing your statistic or a value more extreme, assuming the null hypothesis is true.\n",
    "\n",
    "## P-value Interpretation:\n",
    "\n",
    "- **Small P-value:** If the p-value is small, this suggests that our null hypothesis is not likely to be true. It implies that our observed statistic likely comes from a different distribution than the null.\n",
    "\n",
    "- **Large P-value:** A large p-value indicates that our observed statistic likely comes from the null hypothesis distribution. In such cases, we do not have strong enough evidence to reject the null hypothesis.\n",
    "\n",
    "By comparing our p-value to our predefined Type I error threshold (α), we can decide whether to accept or reject the null hypothesis:\n",
    "\n",
    "- If the p-value ≤ α, we reject the null hypothesis (H0).\n",
    "\n",
    "- If the p-value > α, we fail to reject the null hypothesis (H0).\n",
    "\n",
    "## Formatted Formulas:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{if } p_{val} \\leq \\alpha & \\rightarrow \\text{ Reject } H_0 \\\\\n",
    "\\text{if } p_{val} > \\alpha & \\rightarrow \\text{ Fail to Reject } H_0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Hypothesis Testing Definitions\n",
    "\n",
    "When conducting hypothesis testing, several key terms are involved. Here's a breakdown of these terms and their definitions:\n",
    "\n",
    "- **Type II Error:** This is when we decide there was no change in morale, but there was actually an increase.\n",
    "\n",
    "- **Type I Error:** This occurs when we decide the program increased morale, but it actually did not.\n",
    "\n",
    "- **Null Hypothesis (H0):** This proposes that there was no change in the average morale of the company.\n",
    "\n",
    "- **Alternative Hypothesis (HA):** This suggests that the new program on average increased morale.\n",
    "\n",
    "- **P-value:** This is the probability of observing a change in average morale as extreme as (or more extreme than) the one we've seen, given that there was actually no change in morale.\n",
    "\n",
    "## Shading and P-values\n",
    "\n",
    "When using a distribution to calculate p-values, the shading method will depend on the type of alternative hypothesis:\n",
    "\n",
    "- **Alternative Hypothesis: No difference in average morale:** To calculate the p-value, shade the area greater than the absolute value of the observed statistic and below the negative of the observed statistic.\n",
    "\n",
    "- **Alternative Hypothesis: The average morale after the program is higher:** To calculate the p-value, shade the area greater than the observed statistic.\n",
    "\n",
    "- **Alternative Hypothesis: The average morale after the program is lower:** To calculate the p-value, shade the area less than the observed statistic.\n",
    "\n",
    "# Hypothesis Testing and Decision Making\n",
    "\n",
    "In hypothesis testing, we make a decision based on the likelihood of our data coming from the null hypothesis, given our type I error threshold (α). Importantly, we never _accept_ a hypothesis. Instead, we either **reject** the null hypothesis or **fail to reject** the null hypothesis. This underscores that the null hypothesis is assumed true by default and would be the default choice even if no data were collected.\n",
    "\n",
    "Here are a few examples of conclusions drawn based on p-values and α:\n",
    "\n",
    "| p-value | alpha | Conclusion |\n",
    "|---------|-------|-------------|\n",
    "| 0.03    | 0.05  | Reject the null |\n",
    "| 0.20    | 0.01  | Fail to reject the null |\n",
    "| 0.10    | 0.05  | Fail to reject the null |\n",
    "\n",
    "## Key Terms in Hypothesis Testing\n",
    "\n",
    "Here are the definitions for several key terms used in hypothesis testing:\n",
    "\n",
    "- **alpha (α):** The threshold for type I errors.\n",
    "\n",
    "- **Type I error:** The worst type of error, it occurs when we decide the alternative is true but really the null is true. If the p-value is less than alpha, we reject the null hypothesis, which may lead to a Type I error.\n",
    "\n",
    "- **Type II error:** This happens when we decide the null is true, but really the alternative is true.\n",
    "\n",
    "# Key Terms and Definitions \n",
    "\n",
    "| Term        | Definition |\n",
    "|-------------|------------|\n",
    "| Null Hypothesis | What we assume to be true before collecting any data. |\n",
    "| Alpha | The threshold for the percent of Type I errors we are willing to commit. |\n",
    "| P-value | The probability of observing our statistic or a more extreme statistic from the null hypothesis. |\n",
    "| Confidence Intervals from Bootstrapping Samples | An alternative (and conceptually more simple for most) method of obtaining the results of a hypothesis test. |\n",
    "| Hypothesis Testing & Confidence Intervals | These statistical techniques are generally aimed at helping us understand population parameters. |\n",
    "| Machine Learning | Techniques in this area are aimed at helping us draw conclusions about individuals in our population. |\n",
    "| Bonferroni Correction | One method for correcting our type I error threshold when we perform more than one hypothesis test. |\n",
    "| Statistical Significance | With larger sample sizes, this becomes less relevant. |\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A/B Testing\n",
    "\n",
    "A/B tests are experiments where a control group sees an old version of a web page, and an experiment group sees a new version. A metric is then chosen to measure the level of engagement from users in each group. The results are then used to judge whether one version is more effective than the other.\n",
    "\n",
    "A/B testing can be viewed as a form of hypothesis testing, with the following hypotheses:\n",
    "\n",
    "- **Null Hypothesis**: The new version is no better, or even worse, than the old version.\n",
    "- **Alternative Hypothesis**: The new version is better than the old version.\n",
    "\n",
    "If we fail to reject the null hypothesis, the results would suggest keeping the old version. If we reject the null hypothesis, the results would suggest launching the change.\n",
    "\n",
    "A/B tests can be used for a wide variety of changes, from large feature additions to small adjustments in color, to see what change maximizes your metric the most.\n",
    "\n",
    "However, A/B testing also has its drawbacks:\n",
    "\n",
    "- It can only help you compare two options, but it can't tell you about an option you haven’t considered.\n",
    "- It can produce biased results when tested on existing users due to factors like change aversion and novelty effect.\n",
    "- **Change Aversion**: Existing users may give an unfair advantage to the old version simply because they are unhappy with the change, even if it’s ultimately for the better.\n",
    "- **Novelty Effect**: Existing users may give an unfair advantage to the new version because they’re excited or drawn to the change, even if it isn’t any better in the long run.\n",
    "\n",
    "\n",
    "# A/B Testing Case Study: Audacity\n",
    "\n",
    "**Term**\n",
    "**Description**\n",
    "\n",
    "- **Control**\n",
    "  - The old/current version of a feature being tested\n",
    "- **Experiment**\n",
    "  - The new version of a feature being tested\n",
    "- **Null Hypothesis**\n",
    "  - Experiment does equally or worse than the control\n",
    "- **Alternative Hypothesis**\n",
    "  - Experiment does better than the control\n",
    "\n",
    "In this case study, we'll analyze the A/B test results for Audacity. The customer funnel for typical new users on their site consists of the following stages:\n",
    "\n",
    "1. View home page \n",
    "2. Explore courses \n",
    "3. View course overview page \n",
    "4. Enroll in course \n",
    "5. Complete course\n",
    "\n",
    "Audacity loses users as they go down the stages of this funnel, with only a few making it to the end. To increase student engagement, Audacity is performing A/B tests to try out changes that will hopefully increase conversion rates from one stage to the next.\n",
    "\n",
    "We’ll analyze test results for two changes they have in mind and then make a recommendation on whether they should launch each change.\n",
    "\n",
    "The first change Audacity wants to try is on their homepage. They hope that this new, more engaging design will increase the number of users who explore their courses.\n",
    "\n",
    "The metric we will use is the **click-through rate** (CTR) for the Explore Courses button on the home page. The click-through rate is defined as the number of clicks divided by the number of views. Since Audacity uses cookies, we can identify unique users and make sure we don't count the same one multiple times. For this experiment, we'll define our click-through rate as:\n",
    "\n",
    "CTR: # clicks by unique users / # views by unique users\n",
    "\n",
    "The null and alternative hypotheses are defined as follows:\n",
    "\n",
    "- Null hypothesis (H0): The new homepage design has a click-through rate that is less than or equal to that of the old homepage design.\n",
    "- Alternative hypothesis (H1): The new homepage design has a higher click-through rate than the old homepage design.\n",
    "\n",
    "In a more mathematical format, this can be represented as:\n",
    "\n",
    "- H0: CTRnew - CTRold <= 0\n",
    "- H1: CTRnew - CTRold > 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Supervised and Unsupervised Machine Learning\n",
    "\n",
    "Machine Learning is frequently split into supervised and unsupervised learning.\n",
    "\n",
    "In **supervised machine learning**, you are interested in predicting a label for your data. Commonly, you might want to predict fraud, customers that will buy a product, or home values in an area. Regression, which is an example of supervised machine learning, will be one of our focus areas in this lesson and its extensions in later lessons.\n",
    "\n",
    "On the other hand, in **unsupervised machine learning**, you are interested in clustering data together that isn't already labeled. This is particularly useful in exploratory data analysis, as it allows us to identify structures within the data that we might not have previously known. However, we will not be going into the details of these algorithms in this course.\n",
    "\n",
    "| Description | Term |\n",
    "| :--- | :--- |\n",
    "| A machine learning technique where we are attempting to predict a label based on inputs. | Supervised Learning |\n",
    "| A machine learning technique where we are attempting to group together unlabeled data based on similar characteristics. | Unsupervised Learning |\n",
    "| A key supervised learning technique you will be learning about in this lesson. | Regression (Multiple Linear and Logistic) |\n",
    "\n",
    "| Description | Term |\n",
    "| :--- | :--- |\n",
    "| The variable we are interested in predicting, which is placed on the y-axis. | Response or Dependent Variable |\n",
    "| The variable we are using to predict, which is placed on the x-axis. | Explanatory or Independent Variable |\n",
    "| In simple linear regression we compare two variables of these types: | Quantitative (y) vs. Quantitative (x) |\n",
    "\n",
    "Scatter plots are a common visual for comparing two quantitative variables. A common summary statistic that relates to a scatter plot is the correlation coefficient commonly denoted by r.\n",
    "\n",
    "Though there are a few different ways to measure correlation between two variables, the most common way is with Pearson's correlation coefficient. Pearson's correlation coefficient provides the:\n",
    "\n",
    "- Strength\n",
    "- Direction\n",
    "\n",
    "of a linear relationship. Spearman's Correlation Coefficient does not measure linear relationships specifically, and it might be more appropriate for certain cases of associating two variables.\n",
    "\n",
    "Supporting Materials:\n",
    "\n",
    "- [Different ways to measure correlation](https://video.udacity-data.com/topher/2019/November/5dcb372a_statistics-solutions/statistics-solutions.pdf)\n",
    "- [Pearson's correlation coefficient](https://video.udacity-data.com/topher/2019/November/5dcb373d_pearson-correlation-/pearson-correlation-.pdf)\n",
    "- [Spearman's Correlation Coefficient](https://video.udacity-data.com/topher/2019/November/5dcb373d_pearson-correlation-/pearson-correlation-.pdf)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.20.1.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Your data\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\__init__.py:22\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# numpy compat\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_numpy_dev \u001B[38;5;28;01mas\u001B[39;00m _is_numpy_dev  \u001B[38;5;66;03m# pyright: ignore # noqa:F401\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_libs\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m hashtable \u001B[38;5;28;01mas\u001B[39;00m _hashtable, lib \u001B[38;5;28;01mas\u001B[39;00m _lib, tslib \u001B[38;5;28;01mas\u001B[39;00m _tslib\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\compat\\__init__.py:18\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_typing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m F\n\u001B[1;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     19\u001B[0m     is_numpy_dev,\n\u001B[0;32m     20\u001B[0m     np_version_under1p21,\n\u001B[0;32m     21\u001B[0m )\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyarrow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     23\u001B[0m     pa_version_under1p01,\n\u001B[0;32m     24\u001B[0m     pa_version_under2p0,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     31\u001B[0m     pa_version_under9p0,\n\u001B[0;32m     32\u001B[0m )\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\compat\\numpy\\__init__.py:23\u001B[0m\n\u001B[0;32m     19\u001B[0m     np_percentile_argname \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterpolation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _nlv \u001B[38;5;241m<\u001B[39m Version(_min_numpy_ver):\n\u001B[1;32m---> 23\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     24\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthis version of pandas is incompatible with numpy < \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_min_numpy_ver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     25\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour numpy version is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_np_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     26\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease upgrade numpy to >= \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_min_numpy_ver\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m to use this pandas version\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     27\u001B[0m     )\n\u001B[0;32m     30\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     32\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_np_version\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis_numpy_dev\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     34\u001B[0m ]\n",
      "\u001B[1;31mImportError\u001B[0m: this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.20.1.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Your data\n",
    "data = {\n",
    "    'Temp': [14.2, 16.4, 11.9, 15.2, 18.5, 22.1, 19.4, 25.1, 23.4, 18.1, 22.6, 17.2, 12],\n",
    "    'Sales': [215, 325, 185, 332, 406, 522, 412, 614, 544, 421, 445, 408, 115],\n",
    "    'MPG': [31, 31, 34, 22, 24, 23, 25, 26, 23, 22, 24, 26, 19],\n",
    "    'Weight': [1650, 1800, 1900, 2200, 2250, 2300, 2400, 2500, 2350, 2300, 2400, 2500, 3400]\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Output correlation matrix\n",
    "print(correlation_matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-03T13:52:29.742729800Z",
     "start_time": "2023-08-03T13:52:29.582188400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation Coefficients\n",
    "\n",
    "Correlation coefficients provide a measure of the strength and direction of a linear relationship.\n",
    "\n",
    "We can tell the direction based on whether the correlation is positive or negative.\n",
    "\n",
    "A rule of thumb for judging the strength:\n",
    "\n",
    "| Strong | Moderate | Weak |\n",
    "| :---: | :---: | :---: |\n",
    "| 0.7 ≤ |r| ≤ 1.0 | 0.3 ≤ |r| < 0.7 | 0.0 ≤ |r| < 0.3 |\n",
    "\n",
    "## Calculation of the Correlation Coefficient\n",
    "\n",
    "r = ∑((xi - x̄) * (yi - ȳ)) / √(∑(xi - x̄)² * ∑(yi - ȳ)²)\n",
    "\n",
    "for i = 1 to n\n",
    "\n",
    "It can also be calculated in Excel and other spreadsheet applications using `CORREL(col1, col2)`, where `col1` and `col2` are the two columns you are looking to compare to one another.\n",
    "\n",
    "## A line is commonly identified by an intercept and a slope.\n",
    "\n",
    "- The intercept is defined as the predicted value of the response when the x-variable is zero.\n",
    "\n",
    "- The slope is defined as the predicted change in the response for every one unit increase in the x-variable.\n",
    "\n",
    "We notate the line in linear regression in the following way:\n",
    "\n",
    "ŷ = b₀ + b₁x₁\n",
    "\n",
    "where\n",
    "\n",
    "- ŷ is the predicted value of the response from the line.\n",
    "- b₀ is the intercept.\n",
    "- b₁ is the slope.\n",
    "- x₁ is the explanatory variable.\n",
    "\n",
    "Note that in the equation above, there is a hat in the ŷ term. This hat indicates that the value is the predicted value from the fitted line and it is not the real value. We use y (without the hat) to denote the actual response value for a data point in our dataset.\n",
    "\n",
    "\n",
    "## The main algorithm used to find the best fit line is called the least-squares algorithm.\n",
    "\n",
    "The least-squares algorithm finds the line that minimizes the sum of the squared differences between the observed and predicted values, formally it can be defined as:\n",
    "\n",
    "∑(i=1 to n) (yᵢ - ŷᵢ)²\n",
    "\n",
    "There are other ways we might choose a \"best\" line, but the least squares method tends to do a good job in many scenarios.\n",
    "\n",
    "## The Regression Closed Form Solution\n",
    "\n",
    "You saw in the last video that in regression we are interested in minimizing the sum of the squared differences between the observed and predicted values:\n",
    "\n",
    "∑(i=1 to n) (yᵢ - ŷᵢ)²\n",
    "\n",
    "In order to minimize this function, we can set equations that provide the optimal parameters for the intercept and slope. This process is called the method of ordinary least squares.\n",
    "\n",
    "![Image Description6](https://video.udacity-data.com/topher/2017/November/5a062bc7_screen-shot-2017-11-10-at-2.43.00-pm/screen-shot-2017-11-10-at-2.43.00-pm.png)\n",
    "\n",
    "Let's say we have a set of points and want to compute the slope and intercept of the best fitting line. We would need to compute the following:\n",
    "\n",
    "- x̄ = (1/n) ∑ xᵢ (mean of x)\n",
    "- ȳ = (1/n) ∑ yᵢ (mean of y)\n",
    "- s_y = sqrt[(1/(n-1)) ∑ (yᵢ - ȳ)²] (standard deviation of y using Bessel's correction)\n",
    "- s_x = sqrt[(1/(n-1)) ∑ (xᵢ - x̄)²] (standard deviation of x using Bessel's correction)\n",
    "- r = (1/n) ∑ [(xᵢ - x̄) (yᵢ - ȳ)] / [s_x s_y] (Pearson correlation coefficient)\n",
    "\n",
    "Then, the slope (b₁) and intercept (b₀) of the best fitting line can be calculated as follows:\n",
    "\n",
    "- b₁ = r * s_y / s_x\n",
    "- b₀ = ȳ - b₁ * x̄\n",
    "\n",
    "While you could technically do all of this by hand, in the age of computers it's much more efficient to use software to perform these calculations. This allows us to focus more on interpreting the results and less on the computation. In this lesson, you'll get some practice doing this in Python.\n",
    "\n",
    "## Hypothesis Testing for the Coefficients\n",
    "\n",
    "In linear regression, we can perform hypothesis tests for the coefficients (intercept and slopes) to determine whether these parameters are statistically significant, i.e., it is statistically unlikely that these parameters are zero. If the parameter is significantly different from zero, it means that there is a statistically significant relationship between the predictor variable associated with that coefficient and the response variable.\n",
    "\n",
    "However, the hypothesis test for the intercept (b₀) is usually not of interest since it tests whether the expected value of y when all predictor variables are zero is different from zero.\n",
    "\n",
    "For the slope parameters (bᵢ), the null hypothesis states that the parameter is zero, while the alternative hypothesis states that the parameter is not zero.\n",
    "\n",
    "- H₀: bᵢ = 0 (There is no relationship between the predictor xᵢ and the response y)\n",
    "- H₁: bᵢ ≠ 0 (There is a relationship between the predictor xᵢ and the response y)\n",
    "\n",
    "If the slope is significantly different from zero (i.e., we reject the null hypothesis in favor of the alternative), it indicates that there is a statistically significant linear relationship between the predictor variable and the response. This suggests that the predictor variable does have some predictive power in the context of this model.\n",
    "\n",
    "## R-squared\n",
    "\n",
    "R-squared, also known as the coefficient of determination, measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s). It provides a measure of how well the observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. \n",
    "\n",
    "Mathematically, R-squared is the square of the correlation coefficient (r), and it ranges from 0 to 1. An R-squared of 100 percent indicates that all changes in the dependent variable are completely explained by changes in the independent variable(s).\n",
    "\n",
    "While R-squared is a useful metric, it is not without limitations. It doesn’t indicate whether a regression model is appropriate, and a high R-squared doesn't necessarily mean the model fits the data well. It doesn’t indicate whether the coefficients and predictions are biased, and it doesn’t tell you if you've chosen the right regression.\n",
    "\n",
    "R-squared can be misleading as it tends to increase with the addition of more variables, even if those variables are not truly improving the model. It can lead one to incorrectly believe that a model with more terms is better than a model with fewer terms. This is why some argue that R-squared isn’t always a great measure. It's always a good idea to use other statistics and plots to understand the quality of your model, and cross-validation can assist with validating your chosen measures.\n",
    "\n",
    "Reference: [Is R-squared Useless?](http://data.library.virginia.edu/is-r-squared-useless/)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this lesson, you will be extending your knowledge of simple linear regression, where you were predicting a quantitative response variable using a quantitative explanatory variable. That is, you were using an equation that looked like this:\n",
    "\n",
    "$$\n",
    "\\hat{y}=b_0+b_1x_1\n",
    "$$\n",
    "\n",
    "In this lesson, you will learn about multiple linear regression. In these cases, you will be using both quantitative and categorical x-variables to predict a quantitative response. That is, you will be creating equations that like this to predict your response:\n",
    "\n",
    "$$\n",
    "\\hat{y}=b_0+b_1x_1+b_2x_2+b_3x_3+b_4x_4\n",
    "$$\n",
    "\n",
    "Furthermore, you will learn about how to assess problems that can happen in multiple linear regression, how to address these problems, and how to assess how well your model is performing. It turns out R-squared can be used but might be misleading. And, unfortunately, the correlation coefficient is only a measure of the linear relationship between two quantitative variables, so it will not be very useful in the multiple linear regression case.\n",
    "\n",
    "Here is a wonderful free supplementary book: [Introduction to Statistical Learning](https://www.statlearning.com/). This is an absolutely spectacular book for getting started with machine learning, and Chapter 3 discusses many of the ideas in this lesson. The programming performed in the text is in R, but here is an additional resource, not created by the book's authors, that provides Jupyter Notebooks in Python with notes and answers to nearly all the questions from the book: [Reddit Post With Python Notebooks](https://www.reddit.com/r/learnpython/comments/4skigr/introduction_to_statistical_learning_python/).\n",
    "\n",
    "If you want to fully grasp how the functions we will be using in Python work to fit multiple linear regression models, it is absolutely necessary to have a firm grasp of linear algebra. Though you can complete this class (and fit multiple linear regression models in Python) without knowing linear algebra, linear algebra is useful for understanding why you do or do not obtain certain results from Python, as well as troubleshooting if something goes wrong with fitting your model.\n",
    "\n",
    "Because of your previous work using linear regression models, fitting multiple linear regression models and interpreting their results will feel familiar.\n",
    "\n",
    "Steps to fit a multiple linear regression model:\n",
    "\n",
    "1. Create an intercept\n",
    "2. Use `statsmodels.api` library, as in the previous lesson, but in the x portion add all of the quantitative variables (adding categorical variables will cause an error).\n",
    "3. Fit the model\n",
    "\n",
    "## How Do We Find the \"Right\" Coefficients in Multiple Linear Regression\n",
    "\n",
    "In the simple linear regression section, you saw how we were interested in minimizing the squared distance between each actual data point and the predicted value from our model.\n",
    "\n",
    "But in multiple linear regression, we are actually looking at points that live in not just a two dimensional space.\n",
    "\n",
    "For a full derivation of how this works, [this article](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) provides a breakdown of the steps.\n",
    "\n",
    "The takeaway for us is that we can find the optimal $\\beta$ estimates by calculating $(X'X)^{-1}X'y$.\n",
    "\n",
    "In the following video, you will use `statsmodels` to obtain the coefficients similar to how we did it in the last concept, but you will also solve for the coefficients using the equation above to show the results are not magic.\n",
    "\n",
    "\n",
    "In this video, the coefficients had positive and negative values. Therefore, we can interpret each coefficient as the predicted increase or decrease in the response for every one-unit increase in the explanatory variable, holding all other variables in the model constant.\n",
    "\n",
    "However, in general, coefficients might be positive or negative. Therefore, each coefficient is the predicted change in the response for every one-unit increase in the explanatory variable, holding all other variables in the model constant.\n",
    "\n",
    "This interpretation is very similar to what you saw in the last lesson with the simple addition of the phrase \"holding all other variables constant\" meaning only the variable attached to the coefficient changes, while all other variables stay the same.\n",
    "\n",
    "## Dummy Variables\n",
    "\n",
    "The way that we add categorical variables into our multiple linear regression models is by using dummy variables. The most common way dummy variables are added is through 1, 0 encoding. In this encoding method, you create a new column for each level of a category (in this case A, B, or C). Then our new columns either hold a 1 or 0 depending on the presence of the level in the original column.\n",
    "\n",
    "![Dummy Variables](https://video.udacity-data.com/topher/2017/December/5a297de8_screen-shot-2017-12-07-at-9.43.05-am/screen-shot-2017-12-07-at-9.43.05-am.png)\n",
    "\n",
    "When we add these dummy variables to our multiple linear regression models, we always drop one of the columns. The column you drop is called the baseline. The coefficients you obtain from the output of your multiple linear regression models are then an indication of how the encoded levels compare to the baseline level (the dropped level).\n",
    "\n",
    "## The Math Behind Dummy Variables\n",
    "\n",
    "In the last video, you were introduced to the idea of the way that categorical variables will be changed to dummy variables in order to be added to your linear models.\n",
    "\n",
    "Then, you will need to drop one of the dummy columns in order to make your matrices full rank.\n",
    "\n",
    "If you remember back to the closed-form solution for the coefficients in regression, we have $\\beta$ is estimated by $(X'X)^{-1}X'y$.\n",
    "\n",
    "In order to take the inverse of $(X'X)$, the matrix $X$ must be full rank. That is, all of the columns of $X$ must be linearly independent.\n",
    "\n",
    "If you do not drop one of the columns (from the model, not from the dataframe) when creating the dummy variables, your solution is unstable, and results from Python are unreliable. You will see an example of what happens if you do not drop one of the dummy columns in the next concept.\n",
    "\n",
    "The takeaway is ... when you create dummy variables using 0, 1 encodings, you always need to drop one of the columns from the model to make sure your matrices are full rank (and that your solutions are reliable from Python).\n",
    "\n",
    "The reason for this is linear algebra. Specifically, in order to invert matrices, a matrix must be full rank (that is, all the columns need to be linearly independent). Therefore, you need to drop one of the dummy columns, to create linearly independent columns (and a full rank matrix).\n",
    "\n",
    "\n",
    "To create dummy variables, you can use the Pandas method `pd.get_dummies()`. The documentation for this method can be found [HERE](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html).\n",
    "\n",
    "The `pd.get_dummies()` method—\n",
    "\n",
    "1. Takes in a categorical column\n",
    "2. Creates a new column for each categorical value (in alphabetical order) and assigns values of either 1 or 0, to the rows\n",
    "\n",
    "From previous work, you've seen that fitting categorical columns to linear models, without encoding, does not work.\n",
    "\n",
    "When using dummy variables in our model—\n",
    "\n",
    "1. Always drop the baseline category, one of the encoded columns (which column does not matter)\n",
    "2. The intercept coefficient will be the prediction on the baseline category\n",
    "3. The remaining coefficients are comparisons to the baseline category\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Though Python and R use 1, 0 encodings in the default functionality, there are many ways we might encode dummy variables. SAS is a common programming language that uses a different encoding than you have used so far. Being comfortable with the binary (1, 0) encodings we've been using is really all you need, but if you are curious about other encodings you can try out the quiz on the next page to see how this works.\n",
    "\n",
    "When building multiple linear regression models, there are a number of problems that may arise. However, assessing a \"problem\" will depend on your use case.\n",
    "\n",
    "* Is the focus understanding how all variables are related?\n",
    "* Are you using regression to make predictions about the response variable?\n",
    "* Is determining which variables are most useful for predicting the response, your goal?\n",
    "\n",
    "The problems that can arise are:\n",
    "\n",
    "* A linear relationship may not exist\n",
    "* Correlated errors\n",
    "* Non-constant variance\n",
    "* Outliers\n",
    "* Multicollinearity\n",
    "\n",
    "## Model Assumptions And How To Address Each\n",
    "\n",
    "Here are the five potential problems related to Multiple Linear Regression that we mentioned in the previous video, which are addressed in Introduction to Statistical Learning:\n",
    "\n",
    "* Non-linearity of the response-predictor relationships\n",
    "* Correlation of error terms\n",
    "* Non-constant Variance and Normally Distributed Errors\n",
    "* Outliers/ High leverage points\n",
    "* Multicollinearity\n",
    "\n",
    "This text is a summary of how to identify whether these problems exist, as well as how to address them. This is a common interview question asked by statisticians, but its practical importance is hit or miss depending on the purpose of your model. In the upcoming concepts, we will look more closely at specific points that I believe deserve more attention, but below you will see a more exhaustive introduction to each topic. Let's take a closer look at each of these items.\n",
    "\n",
    "**Linearity**\n",
    "\n",
    "The assumption of linearity is that a linear model is a relationship that truly exists between your response and predictor variables. If this isn't true, then your predictions will not be very accurate. Additionally, the linear relationships associated with your coefficients really aren't useful either.\n",
    "\n",
    "In order to assess if a linear relationship is reasonable, a plot of the residuals (y−y^) by the predicted values (y^) is often useful. If there are curvature patterns in this plot, it suggests that a linear model might not actually fit the data, and some other relationship exists between the predictor variables and response. There are many ways to create non-linear models (even using the linear model form), and you will be introduced to a few of these later in this lesson.\n",
    "\n",
    "In the image at the bottom of this page, these are considered biased models. Ideally, we want to see a random scatter of points like the top left residual plot in the image.\n",
    "\n",
    "**Correlated Errors**\n",
    "\n",
    "Correlated errors frequently occur when our data are collected over time (like in forecasting stock prices or interest rates in the future) or data are spatially related (like predicting flood or drought regions). We can often improve our predictions by using information from the past data points (for time) or the points nearby (for space).\n",
    "\n",
    "The main problem with not accounting for correlated errors is that you can often use this correlation to your advantage to better predict future events or events spatially close to one another.\n",
    "\n",
    "One of the most common ways to identify if you have correlated errors is based on the domain from which the data were collected. If you are unsure, there is a test known as a Durbin-Watson test that is commonly used to assess whether the correlation of the errors is an issue. Then ARIMA or ARMA models are commonly implemented to use this correlation to make better predictions.\n",
    "\n",
    "**Non-constant Variance and Normally Distributed Errors**\n",
    "\n",
    "Non-constant variance is when the spread of your predicted values differs depending on which value you are trying to predict. This isn't a huge problem in terms of predicting well. However, it does lead to confidence intervals and p-values that are inaccurate. Confidence intervals for the coefficients will be too wide for areas where the actual values are closer to the predicted values but too narrow for areas where the actual values are more spread out from the predicted values.\n",
    "\n",
    "Commonly, a log (or some other transformation of the response variable is done) in order to \"get rid\" of the non-constant variance. In order to choose the transformation, a Box-Cox is commonly used.\n",
    "\n",
    "Non-constant variance can be assessed again using a plot of the residuals by the predicted values. In the image at the bottom of the page, non-constant variance is labeled as heteroscedastic. Ideally, we want an unbiased model with homoscedastic residuals (consistent across the range of values).\n",
    "\n",
    "Though the text does not discuss the normality of the residuals, this is an important assumption of regression if you are interested in creating reliable confidence intervals. More on this topic is provided here.\n",
    "\n",
    "**Outliers/Leverage Points**\n",
    "\n",
    "Outliers and leverage points are points that lie far away from the regular trends of your data. These points can have a large influence on your solution. In practice, these points might even be typos. If you are aggregating data from multiple sources, it is possible that some of the data values were carried over incorrectly or aggregated incorrectly.\n",
    "\n",
    "Other times outliers are accurate and true data points, not necessarily measurement or data entry errors. In these cases, 'fixing' is more subjective. Often the strategy for working with these points is dependent on the goal of your analysis. Linear models using ordinary least squares, in particular, are not very robust. That is, large outliers may greatly change our results. There are techniques to combat this - largely known as regularization techniques. These are beyond the scope of this class, but they are quickly discussed in the free course on machine learning.\n",
    "\n",
    "**Multi-collinearity**\n",
    "\n",
    "Multicollinearity is when we have predictor variables that are correlated with one another. One of the main concerns of multicollinearity is that it can lead to coefficients being flipped from the direction we expect from simple linear regression.\n",
    "\n",
    "One of the most common ways to identify multicollinearity is with bivariate plots or with variance inflation factors (or VIFs). This is a topic we will dive into in the next concept, so we won't spend as much time on it here.\n",
    "\n",
    "![image](https://video.udacity-data.com/topher/2017/November/5a0d2a76_resid-plots/resid-plots.gif)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "Multicollinearity occurs when predictor variables in a regression model are correlated with each other. This can lead to problematic outcomes such as flipped relationships between predictor variables and the response and less stable coefficient estimates.\n",
    "\n",
    "There are two main ways to identify multicollinearity:\n",
    "\n",
    "1. **Correlation Analysis:** Examine the correlation between each pair of explanatory variables. This can be done using correlation coefficients or visually using a plot.\n",
    "2. **Variance Inflation Factors (VIFs):** VIFs measure how much the variance of the estimated regression coefficients is increased due to multicollinearity. VIFs greater than 10 (or even 5, according to some experts) can indicate problematic multicollinearity.\n",
    "\n",
    "Refer to this [post](https://etav.github.io/python/vif_factor_python.html) for more on VIFs and multicollinearity.\n",
    "\n",
    "## Dealing with Multicollinearity\n",
    "\n",
    "The most common strategy to handle multicollinearity is to remove one of the variables that is most correlated with others. The choice of variable to remove often depends on its importance in your analysis. However, this strategy should be applied with caution as it might lead to loss of important information.\n",
    "\n",
    "# Adding Higher Order Terms in Linear Models\n",
    "\n",
    "We can make linear models approximate non-linear relationships by including higher-order terms:\n",
    "\n",
    "1. **Interactions:** Product of two or more variables.\n",
    "2. **Quadratics:** Squared terms of variables.\n",
    "3. **Cubics:** Cubed terms of variables.\n",
    "4. **Higher order values:** Higher powers of variables.\n",
    "\n",
    "While this increases the model's predictive power, it complicates interpretation.\n",
    "\n",
    "## Identifying Higher-Order Terms\n",
    "\n",
    "Higher-order terms are created when multiplying two or more predictor variables. This includes quadratics (like $x_1^2$), cubics (like $x_1^3$), and interactions (like $x_1x_2$).\n",
    "\n",
    "For instance, consider a simple linear model:\n",
    "\n",
    "$ \\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2$\n",
    "\n",
    "To include higher order terms, the model could change to:\n",
    "\n",
    "$ \\hat{y} = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\beta_3x_2 + \\beta_4x_1x_2$\n",
    "\n",
    "Here, we've introduced a quadratic term ($\\beta_2x_1^2$) and an interaction term ($\\beta_4x_1x_2$).\n",
    "\n",
    "To decide whether to add quadratic, cubic, or higher order terms, examine the relationships between the predictor and response variables:\n",
    "\n",
    "- **Quadratic term:** Useful when there's one curve in the relationship.\n",
    "- **Cubic term:** Useful when there are two curves in the relationship.\n",
    "\n",
    "![Quadratic Relationship](https://video.udacity-data.com/topher/2017/January/58868097_quadraticlinearregression/quadraticlinearregression.png)\n",
    "\n",
    "_Example of a quadratic relationship_\n",
    "\n",
    "![Cubic Relationship](https://video.udacity-data.com/topher/2017/December/5a25c33e_resid2/resid2.jpg)\n",
    "\n",
    "_Example of a cubic relationship ([source](https://tamino.wordpress.com/2011/03/31/so-what/))_\n",
    "\n",
    "# Higher Order Terms and Interactions in Linear Models\n",
    "\n",
    "At times, we may need to model non-linear relationships between the response and explanatory variables. This can be achieved by introducing higher order terms and interaction terms.\n",
    "\n",
    "## Higher Order Terms\n",
    "\n",
    "Higher order terms are created by taking powers of explanatory variables. For example, to model a quadratic relationship with the variable 'bedrooms', we square the 'bedrooms' column values. Note that when higher order terms are added, their corresponding lower order terms should be included in the model as well.\n",
    "\n",
    "## Interaction Terms\n",
    "\n",
    "Interaction terms are created by multiplying two or more predictor variables. These terms can significantly improve the model's $R^2$ value as they account for the combined effect of the interacting variables.\n",
    "\n",
    "# Interpreting Interactions\n",
    "\n",
    "An interaction term $(x_1x_2)$ indicates that the relationship between the response variable $(y)$ and a variable $(x_1)$ depends on the value of another variable $(x_2)$.\n",
    "\n",
    "For instance, consider a model predicting house prices $(y)$ based on total square footage $(x_1)$ and neighborhood $(x_2)$. Here, the relationship between house price and square footage may differ across neighborhoods. The slope of the relationship $(x_1, y)$ varies depending on the neighborhood.\n",
    "\n",
    "- **No interaction:** If the slopes of the $(x_1, y)$ lines for different $(x_2)$ values are equal or nearly equal, there's no need to add an interaction term.\n",
    "- **Interaction:** If the slopes are not equal, this suggests the presence of an interaction effect, and an interaction term should be added to the model.\n",
    "\n",
    "![im17](./images/17.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Interaction Terms and their Interpretation\n",
    "\n",
    "Interaction terms are created by multiplying two variables together. These are especially useful when the relationship between two explanatory variables and the response variable isn't strictly additive.\n",
    "\n",
    "Consider a model where we predict home prices based on area (x₁) and neighborhood (x₂). A simple linear model without an interaction term may look like:\n",
    "\n",
    "$ŷ = b₀ + b₁x₁ + b₂x₂$\n",
    "\n",
    "Here, $b₁$ is the slope between area and price, assumed to be the same regardless of the neighborhood. $b₂$ is the difference in price between the two neighborhoods, assumed to be constant regardless of the area.\n",
    "\n",
    "![No Interaction Plot](https://video.udacity-data.com/topher/2017/December/5a29d393_screen-shot-2017-12-07-at-3.48.20-pm/screen-shot-2017-12-07-at-3.48.20-pm.png)\n",
    "\n",
    "This model works well when the relationship between area and price is the same for all neighborhoods, and the price difference between neighborhoods is constant across all areas.\n",
    "\n",
    "However, if the relationship between area and price varies by neighborhood, we should include an interaction term. In mathematical terms, this gives us:\n",
    "\n",
    "$ŷ = b₀ + b₁x₁ + b₂x₂ + b₃x₁x₂$\n",
    "\n",
    "The interaction term ($b₃x₁x₂$) allows for different slopes for each neighborhood, representing different price-to-area relationships for each one.\n",
    "\n",
    "![Interaction Plot](https://video.udacity-data.com/topher/2018/April/5ae5fcb3_screen-shot-2018-04-29-at-10.10.52-am/screen-shot-2018-04-29-at-10.10.52-am.png)\n",
    "\n",
    "This suggests that the effect of area on price is not simply additive, but depends on the neighborhood as well. If the lines representing the relationship between price and area for different neighborhoods cross or diverge significantly, an interaction term is warranted.\n",
    "\n",
    "# Lesson Recap\n",
    "\n",
    "In this lesson, we covered:\n",
    "\n",
    "1. **Multiple Linear Regression Model**: Building a multiple linear regression model in Python, which is similar to building a simple linear regression model.\n",
    "\n",
    "2. **Dummy Variables**: Encoding and interpreting dummy variables and their coefficients.\n",
    "\n",
    "3. **Higher-Order Terms**: Creating and analyzing higher-order terms and understanding their impact on interpreting coefficients.\n",
    "\n",
    "4. **Interaction Terms**: Identifying the need for interaction terms in a multiple linear regression model. These increase the predictive power of the model but may make interpreting coefficients less straightforward.\n",
    "\n",
    "5. **Model Assumptions and Multicollinearity**: Evaluating model assumptions, understanding multicollinearity, and its impact on model coefficients and standard errors. This includes learning about variance inflation factors.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression predicts the relationship between categorical dependent variables and predictors. Unlike linear regression which predicts a continuous outcome, logistic regression predicts categorical responses, specifically when there are only two possible outcomes.\n",
    "\n",
    "## Logistic Regression Equations\n",
    "\n",
    "Unlike linear regression, where the predicted response can take any value, the predicted response in logistic regression is constrained to a probability between 0 and 1.\n",
    "\n",
    "### Mathematical Representation\n",
    "\n",
    "- The categorical column labels (limited to two values) must be encoded with values of 0 and 1.\n",
    "- A linear model predicts the log odds instead of predicting the response itself. If `p` is the probability of a one value occurring, we can calculate the odds ratio as:\n",
    "\n",
    "```\n",
    "log(p / (1 - p)) = b0 + b1*x1 + b2*x2 + ...\n",
    "```\n",
    "\n",
    "- The predicted probabilities of success (from 0 to 1) are obtained by transforming the log odds using the logistic function:\n",
    "\n",
    "```\n",
    "p = e^(b0 + b1*x1 + b2*x2 + ...) / (1 + e^(b0 + b1*x1 + b2*x2 + ...))\n",
    "```\n",
    "\n",
    "## Comparison of Logistic and Linear Regression\n",
    "\n",
    "- **Logistic Regression** predicts a probability between 0 and 1.\n",
    "- **Linear Regression** predicts any value between negative and positive infinity.\n",
    "- For both types, explanatory variables can be either categorical or quantitative.\n",
    "\n",
    "# Steps for Fitting Logistic Regression in Python\n",
    "\n",
    "1. **Load Libraries**:  \n",
    "Similar to working with linear regression models, the `statsmodels.api` library is essential for running logistic regression models.\n",
    "\n",
    "2. **Handle Categorical Columns**:  \n",
    "Ensure you transform categorical columns into dummy variables.\n",
    "\n",
    "3. **Impute Missing Values**:  \n",
    "Deal with any missing data by implementing suitable imputation strategies.\n",
    "\n",
    "4. **Specify Model**:  \n",
    "Use the `Logit()` function from `statsmodels.api`. The parameters for this function are:\n",
    "    - Response variable\n",
    "    - Intercept\n",
    "    - Explanatory variable, `x`.\n",
    "\n",
    "5. **Fit The Model**:  \n",
    "Call the `fit()` function on the specified model to train it on your data.\n",
    "\n",
    "6. **Model Summary**:  \n",
    "Obtain a summary of the model's performance and parameters using the `summary()` function.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mstatsmodels\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msm\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#dummy variables\u001B[39;00m\n\u001B[1;32m----> 6\u001B[0m df[[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno_fraud\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfraud\u001B[39m\u001B[38;5;124m'\u001B[39m]]\u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mget_dummies(\u001B[43mdf\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfraud\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m      7\u001B[0m df\u001B[38;5;241m=\u001B[39mdf\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno_fraud\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#instantiate logistic regression model\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#dummy variables\n",
    "df[['no_fraud', 'fraud']]= pd.get_dummies(df['fraud'])\n",
    "df=df.drop('no_fraud', axis=1)\n",
    "\n",
    "#instantiate logistic regression model\n",
    "df['intercept']= 1\n",
    "logit_mod =sm.Logit(df['fraud'],df[['intercept', 'duration']])\n",
    "\n",
    "#fit the model\n",
    "results = logit_mod.fit()\n",
    "\n",
    "#get summary statistics\n",
    "results.summary() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T08:35:39.338206400Z",
     "start_time": "2023-08-04T08:35:10.365652600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#to exponentiate\n",
    "np.exp(-1.4637), np.exp(2.5465)\n",
    "\n",
    "# Interpreting Logistic Regression Results\n",
    "\n",
    "## Part I\n",
    "\n",
    "| Variable  | Coefficient | Standard Error | t-value | P-value | 95% CI |\n",
    "|---|---|---|---|---|---|\n",
    "| Intercept | 9.8709 | 1.944 | 5.078 | 0.000 | 6.061, 13.691 |\n",
    "| Duration  | -1.4637 | 0.290 | - | 0.000 | -2.033, -0.894 |\n",
    "| Weekday   | 2.5465 | 0.904 | - | 0.000 | 0.774, 4.319 |\n",
    "\n",
    "**Quantitative Interpretations:**  \n",
    "For every one unit increase in the explanatory variable `x1`, we expect a multiplicative change in the odds of being in the 1 category of `eb1`, holding all other variables constant.\n",
    "\n",
    "**Categorical Interpretations:**  \n",
    "When in category `x1`, we expect a multiplicative change in the odds of being in the 1 category of `eb1`, compared to the baseline.\n",
    "\n",
    "## Part II\n",
    "\n",
    "| Variable  | Coefficient | Standard Error | t-value | P-value | 95% CI |\n",
    "|---|---|---|---|---|---|\n",
    "| Intercept | 9.8709 | 1.944 | 5.078 | 0.000 | 6.061, 13.691 |\n",
    "| Duration  | -1.4637 | 0.290 | - | 0.000 | -2.033, -0.894 |\n",
    "| Weekday   | 2.5465 | 0.904 | - | 0.000 | 0.774, 4.319 |\n",
    "\n",
    "We exponentiate the coefficients for `duration` (-1.4637) and `weekday` (2.5465) to get the approximate values of 0.231 and 12.762, respectively. \n",
    "\n",
    "The result is interpreted as follows:\n",
    "\n",
    "- For each 1 unit increase in duration, fraud is 0.23 times as likely, holding all other variables (weekday) constant.\n",
    "- Fraud is 12.76 times as likely on weekdays than weekends, holding all other variables (duration) constant.\n",
    "\n",
    "> **Note:** When multiplicative changes are less than 1, like duration = 0.231, it is usually useful to calculate the reciprocal. This changes the direction from a unit increase to a unit decrease. So, the result for duration could also be interpreted as:\n",
    "\n",
    "- For every 1 unit decrease in duration, fraud is 4.32 times as likely, holding all other variables constant.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Performance Metrics for Classification Models\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "The most common measure to evaluate a model's performance is **accuracy**.\n",
    "\n",
    "The formula for accuracy is:\n",
    "\n",
    "\\[\n",
    "\\text{Accuracy} = \\frac{\\text{Number of correctly labeled rows}}{\\text{Number of total rows in the dataset}}\n",
    "\\]\n",
    "\n",
    "However, accuracy is not always the best measure, especially for unbalanced datasets. Therefore, we will cover other types of metrics as well.\n",
    "\n",
    "## Confusion Matrices\n",
    "\n",
    "A confusion matrix is a 2x2 matrix grid that compares actual class to the predicted class.\n",
    "\n",
    "| Predicted Class \\ Actual Class | Positive | Negative |\n",
    "|-----------------------|----------|----------|\n",
    "| Positive              | Pos,Pos  | Neg,Pos  |\n",
    "| Negative              | Pos,Neg  | Neg,Neg  |\n",
    "\n",
    "Confusion matrices are useful in various contexts, such as the Eigenfaces example shown below.\n",
    "\n",
    "### Confusion Matrix for Eigenfaces\n",
    "\n",
    "| True/Actual | Ariel Sharon | Colin Powell | Donald Rumsfeld | George W Bush | Gerhard Schroeder | Hugo Chavez | Tony Blair |\n",
    "|-------------|--------------|--------------|-----------------|---------------|-------------------|-------------|------------|\n",
    "| Ariel Sharon | 13 | 4 | 1 | 1 | 0 | 0 |\n",
    "| Colin Powell | 0 | 55 | 0 | 8 | 0 | 0 |\n",
    "| Donald Rumsfeld | 0 | 1 | 25 | 8 | 0 | 0 |\n",
    "| George W Bush | 0 | 3 | 0 | 123 | 0 | 0 |\n",
    "| Gerhard Schroeder | 0 | 1 | 0 | 7 | 14 | 0 |\n",
    "| Hugo Chavez | 0 | 3 | 0 | 2 | 1 | 10 |\n",
    "| Tony Blair | 0 | 0 | 1 | 7 | 0 | 0 |\n",
    "\n",
    "The images of seven different male politicians were put through an eigenface analysis, to determine and vectorize the principal components of the dataset and then reuse the vectors to remap new faces to names.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Recall and Precision\n",
    "\n",
    "## Definitions\n",
    "\n",
    "- **Recall:**\n",
    "True Positive / (True Positive + False Negative).\n",
    "Out of all the items that are truly positive, how many were correctly classified as positive. Or simply, how many positive items were 'recalled' from the dataset.\n",
    "\n",
    "- **Precision:**\n",
    "True Positive / (True Positive + False Positive).\n",
    "Out of all the items labeled as positive, how many truly belong to the positive class.\n",
    "\n",
    "## Precision and Recall Example\n",
    "\n",
    "Consider the following confusion matrix for seven politicians:\n",
    "\n",
    "| True/Actual | Ariel Sharon | Colin Powell | Donald Rumsfeld | George W Bush | Gerhard Schroeder | Hugo Chavez | Tony Blair |\n",
    "|-------------|--------------|--------------|-----------------|---------------|-------------------|-------------|------------|\n",
    "| Ariel Sharon | 13 | 4 | 1 | 1 | 0 | 0 | 1 |\n",
    "| Colin Powell | 0 | 55 | 0 | 8 | 0 | 0 | 0 |\n",
    "| Donald Rumsfeld | 0 | 1 | 25 | 8 | 0 | 0 | 2 |\n",
    "| George W Bush | 0 | 3 | 0 | 123 | 0 | 0 | 1 |\n",
    "| Gerhard Schroeder | 0 | 1 | 0 | 7 | 14 | 0 | 4 |\n",
    "| Hugo Chavez | 0 | 3 | 0 | 2 | 1 | 10 | 0 |\n",
    "| Tony Blair | 0 | 0 | 1 | 7 | 0 | 0 | 26 |\n",
    "\n",
    "For example:\n",
    "\n",
    "- The **recall** rate for Hugo Chavez is the probability of the algorithm classifying an image as Hugo Chavez and the person is indeed Hugo Chavez. This was 10/16 or 0.625.\n",
    "- The **precision** rate for Hugo Chavez is the probability that the algorithm correctly identifies the image as Hugo Chavez. This was 10/10 or 1.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/admissions.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 10\u001B[0m\n\u001B[0;32m      7\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m#read in dataset\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./data/admissions.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#create response and explanatory variables\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1733\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1734\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1738\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1739\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1740\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1741\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1742\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32mD:\\Users\\Chait\\anaconda3\\envs\\Krishna\\lib\\site-packages\\pandas\\io\\common.py:856\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    852\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    855\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/admissions.csv'"
     ]
    }
   ],
   "source": [
    "#read in libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(42)\n",
    "\n",
    "#read in dataset\n",
    "df = pd.read_csv('./data/admissions.csv')\n",
    "df.head()\n",
    "\n",
    "#create response and explanatory variables\n",
    "y = df['admit']\n",
    "df[['level1','level2','level3','level4']] = pd.get_dummies(df['prestige'])\n",
    "X = df[['gre', 'gpa','level1','level2','level3']]\n",
    "\n",
    "#create the train test split\n",
    "X_train, X_test, y_train, y_test =train_test_split(X,y,test_size=0.10, random_state=42)\n",
    "\n",
    "#instantiate logistic regression model\n",
    "log_mod = LogisticRegression()\n",
    "\n",
    "#fit the model to the training data\n",
    "log_mod.fit(X_train, y_train)\n",
    "\n",
    "#create predictions using test data\n",
    "y_preds = log_mod.predict(X_test)\n",
    "\n",
    "#print summary statistics\n",
    "print(precision_score(y_test, y_preds))\n",
    "print(recall_score(y_test, y_preds))\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-04T09:20:29.927466600Z",
     "start_time": "2023-08-04T09:20:24.765752100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Precision Formula and Train-Test Split\n",
    "\n",
    "## Precision Formula\n",
    "\n",
    "Precision can be defined as the ratio of true positives to the sum of true positives and false positives.\n",
    "\n",
    "\\[\n",
    "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "\\]\n",
    "\n",
    "Consider the following confusion matrix:\n",
    "\n",
    "| Actual \\ Predicted | 0 | 1 |\n",
    "|----------------|---|---|\n",
    "| 0 | 23 | 1 |\n",
    "| 1 | 14 | 2 |\n",
    "\n",
    "This matrix translates to:\n",
    "\n",
    "- 23 non-admitted that we predict to be non-admitted.\n",
    "- 14 admitted that we predicted to be non-admitted.\n",
    "- 1 non-admitted that we predict to be admitted.\n",
    "- 2 admitted that we predict to be admitted.\n",
    "\n",
    "## Why Train-Test Split\n",
    "\n",
    "Creating a train and test dataset is a common approach in machine learning. Splitting your data into training and testing data ensures that your model can predict well not only with the data it was fit to, but also on data that the model has never seen before. Proving the model performs well on test data assures that you have a model that will do well in future use cases - whether that be future students, future transactions, or any other future predictions you might want to make.\n",
    "\n",
    "## Additional Documentation\n",
    "\n",
    "- [Logistic Regression in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "- [Confusion Matrix in sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- [Train-Test Split: Rationale and Process](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
